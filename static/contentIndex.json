{"Bits/Atlas-Is-the-Best-DB-Migration-Tool":{"title":"Atlas Is the Best DB Migration Tool","links":[],"tags":[],"content":"Ariga’s Atlas database migration tool is hands-down THE best migration tool out there. The way it works is wonderfully simple and innovative.\nThe main advantage of Atlas over other tools is that it allows a “declarative workflow”. This means that you no longer need to write migrations as SQL scripts that need to be executed in a particular order, as in Flyway or Liquibase. Rather, you define the schema you desire, and Atlas figures out how to get the production schema to that desired state.\nWhat I consider an incredible UX hack though is that Atlas allows you to define your schema in raw SQL, unlike tools like Prisma that make you learn their own schema definition language that doesn’t work as well as SQL. Not even by a long shot.\nIt does all this with a simple trick: it runs your imperative SQL schema definition script on a dummy database, then computes the difference between the dummy database and the production database. Based on the diff, it decides which tables and columns it needs to modify in order to make the production DB look like the dummy DB.\nThis approach makes so much sense! It simply boggles the mind that most of us are still clumsily creating  versioned migration files and dealing with stored file hashes of failed migrations. Atlas’s way makes changes to the DB schema similar to any code change: you just open the schema.sql file, and edit the table you want, or add a CREATE TABLE statement to the end of the file.\n”Declarative workflow” fits much better with the way we work nowadays. In code reviews, for example, the reviewer looks at migration files and can’t tell what the previous state was. There’s no Git diff! It’s a new file! Another example is rollbacks: you revert the commit with the bad migration, and you’re done! We don’t need two separate versioning systems - Git, and whatever the migration tool does with tables. Git is fine."},"Bits/Do-Not-Re-Invent-the-Wheel":{"title":"Do Not Re-Invent the Wheel","links":["Bits/Mindless-Learning"],"tags":[],"content":"You’re a developer, and you’ve likely heard this before: Don’t re-invent the wheel. That’s damn good advice. It keeps surprising me how deeply wise these four words are - the product of thousands of hours sunk in solutions to already-solved problems.\nI had nobody to tell me this when I started. So here’s my take on the matter.\nIn software engineering, you shouldn’t immediately jump to writing code. Don’t re-invent the wheel. Instead, look at the tools others are using. Almost no problem I’ve faced in my career is distinct, as I’m sure is the case for most software engineers.\nSomeone out there on the internet already had a similar problem to yours, and if enough people had the same problem, you’ll likely find a ready-made solution. You just need to get better at research and understanding the value of things.\nThere’s a good chance you believe your problems are special, and thus require a unique solution. They are not. Most probably, what you’re thinking about has already been invented, and even has its own name. Either that, or your problem can be solved by combining multiple solutions that were also already invented.\nWhen taken to an extreme, this elevator of thought takes us down to Linux, the operating system that integrates your hardware and software together, among many, many other things. Should you re-invent an operating system? No, you use Linux.\nHowever, day-to-day development doesn’t involve thinking of problems down at that level. Examples of things you already have, or will think of re-inventing include:\n\nAuth\nData serialization formats\nTask scheduling\nBatch processing\nStream processing\nData integration\nWorkflow orchestration\nContainer orchestration\nApplication observability\nData schemas\nEmbedded languages\n\nPlease don’t. Just look it up. Google it with absolute confidence that you’ll find what you need as an open source project, or reasonably priced SaaS. Some mindless learning can help as well.\nAlso, don’t forget to thoroughly check everything your current cloud provider offers. You may be surprised."},"Bits/How-two-WhatsApp-Chat-Bots-had-a-Fight":{"title":"How two WhatsApp Chat Bots had a Fight","links":[],"tags":[],"content":"Would you like to know how two Whatsapp chat bots had a fight?\nBeing an engineer at a tech startup, you get to witness many strange occurrences. This is one of the funniest I’ve come across.\nBusinessChat.io is a customer support and marketing platform over Whatsapp. We have a chat bot engine, and a marketing campaigns module that lets our users send a message to tens of thousands of their customers with the click of a button. Our user uploads a list of contacts, assigns them to a group, and reaches out to all of them simply by picking a message template, and hitting “Send”.\nWhen a contact replies to a marketing message, they enter a conversation with the chat bot. Whatsapp bots usually reply with quick-reply messages, which are interactive messages that have buttons the contact can tap to let the bot know what they want. So for example, the bot might ask the contact: “How can I help you today?”, with buttons saying “Talk to customer support” and “Find our nearest branch”.\nWith BusinessChat, if the bot prompts you to tap a button and you respond with something it doesn’t understand (i.e. text different from any of the buttons it expects you to click) the bot will reply to you with “Sorry, I don’t understand” and will once again prompt you to click one of the buttons.\nSo, one peaceful day, a colleague of mine taps me on the shoulder and says “There’s a conversation that takes forever to load.”\nFor some context, in BusinessChat, a conversation is a sequence of events and messages exchanged between a business and one of its contacts. You can load the conversation for a specific contact in the UI. Since the beginning of the project, we thought these messages and events weren’t worth paginating or lazy-loading, since the longest conversation ever made is about 500 messages long, and messages aren’t large at all. So for the longest time now, conversation loading without pagination has been fast and cheap.\nMy colleague and I began our investigation. We started by checking out the number of messages in the troublesome conversation, and to our surprise, it had 90,000 messages.\nMy mind started generating hypotheses. “Is there a bug in the retry logic that infinitely tries to send a failed message? No, it’s hard-coded to retry 3 times only” and “Is there an infinite loop in one of the bots? Impossible, I validate that bots have no invalid cycles.”\nAbout five stressful minute into the investigation, all doubts were erased when we saw the actual content of the conversation. It was entirely made of messages saying “Sorry, I don’t understand”, sent not only from our bot, but also the other number. It was two bots telling each other they couldn’t understand one another 45,000 times.\nWhat triggered this battle of bots was that one of our users had imported a list of contacts where one of the numbers was connected to a bot. When the user started a marketing campaign, and sent a message to this number, war began and lasted 3 hours until, presumably, someone turned off the other bot.\nI would like to believe that someone really did notice the infinite loop and turned off the bot. The alternative would be that our bot took down their service, which I really hope wasn’t the case."},"Bits/I-Hope-Swift-Wins":{"title":"I Hope Swift Wins","links":[],"tags":[],"content":"Apple’s Swift is shaping up to be potentially one of the best languages for writing server-side apps.\nIt seems Apple is really pushing the language in ways that make it more accessible to more developers. Writing a “Hello World” in Swift shouldn’t be so different an experience from using Golang or C#. You just use VSCode on whatever platform you’re using, and run your program from the commandline. It’s all becoming too familiar.\nBut they’re not only working on developer tooling. They’re heavily investing in the library ecosystem, with core functions like observability and OpenAPI generators being maintained by Apple themselves.\nSwift just feels like a modern language. It has a standard package manager that uses Git for versioning (similar to Go). It has compile-time macros, and very expressive types, control flow, and concurrency primitives. It’s also praised for having C-like performance, but without the low-level toil as with Rust.\nHonestly, I was surprised when I realized how mature the backend ecosystem is. I actually hope Swift ends up in the mainstream instead of Go and Rust. It’s much better balanced for productivity, safety, and efficiency.\nWatch the WWDC 24 session."},"Bits/Mindless-Learning":{"title":"Mindless Learning","links":[],"tags":[],"content":"One useful approach to studying any field is to mindlessly consume related content until you are able to criticize, and rationalize its basic concepts, then dig deeper. Sounds absurd? Here’s how it works.\nWhen talking about learning new concepts, we often use the metaphor ‘connecting the dots,’ which I’ll be abusing here. This mindless approach to learning focuses on collecting dots to connect when starting to study a field. You collect dots by reading articles, or watching videos with little to no deep thought. You then connect the dots when you know more about each dot, or where it fits in the graph of dots.\nLet’s take software engineering as an example, since it’s one of the fields where I’ve seen this approach in effect. When starting out, maybe after taking a university course in programming basics, you have no idea how to get from what you’ve learned (the seemingly trivial ‘if’ statements and ‘for’ loop) to building complex, useful software. An introductory programming course will not introduce you to any ways of building interactive graphical interfaces, or HTTP servers.\nFirst, we identify the content to mindlessly consume. There exist many YouTube channels that regularly upload technical conference talks and tutorials. Follow them. Watch every video they upload. While having lunch, taking a break, or on the bus, watch them without much thought.\nThere also exist many technical articles posted on websites such as news.ycombinator.com every day. Read them mindlessly as well. With time, you’ll be able to tell which articles are relevant to your bag of software engineering dots.\nAfter mindlessly watching videos and reading articles for a while, you start finding yourself using words you’ve heard or read in the content you’ve consumed. You find yourself curious about a few particular words that you often search for, and learn more about. You start to build an index of solutions to technical problems, except the index takes you to a web page or a video, and you’ll have to dig deeper from there in order to actually solve the problem.\nWhen you’ve used your problem index a few times, you’ll find you’ve added a few technical tools to your belt. You now can solve problems A, B and C with the tools X, Y, and Z.\nIn many fields, you hear experts saying ‘an expert is a practitioner with many tools on his belt.’ So congratulations, you’re on the path to becoming an expert. You just need to find more efficient methods of learning, now that you have a sufficiently large bag of connected dots."},"Blog/A-Guide-to-Scala-3":{"title":"A Guide to Scala 3","links":[],"tags":[],"content":"\nScala 3 comes with many amazing new features. This article attempts to explain the most notable ones, so it is by no means comprehensive. It is, however, a very good introduction to these concepts for beginner to intermediate-level Scala programmers.\nAt the time of this writing, Scala 3 isn’t actually officially released. However, all the features that would be in Scala 3 are available in Dotty, so we’ll be using it instead.\nThe examples in this article are on GitHub, and in order to run them, you can clone the repository and run sbt console:\ngit clone github.com/m-tabaza/guide-to-scala3-examples.git  \ncd guide-to-scala3-examples/  \nsbt console\n\nIf you use Visual Studio Code, and you have it on the PATH (i.e. the code command is available globally,) you can run sbt launchIDE to get a better editing experience.\n\nIntersection Types\nConsider the following definitions:\ntrait A  {  \n  val a: String  \n}\n \ntrait B {  \n  val b: Int  \n}\nIf we think of types in terms of sets, a type would be a collection of elements that satisfy certain properties. For an element to be a member of type A, it has to have the property a of type String. The same goes for elements of type B.\nSince types are just sets, what would the intersection of A and B be? Well, it’s the set of elements that satisfy the properties of both A and B. We can denote such a type as A &amp; B (the intersection type of A and B,) and we can use it to specify data types such as the type of the argument c in:\ndef f(c: A &amp; B) = c.a + &quot; &amp; &quot; + c.b\nWe can create an instance of A &amp; B by creating a subtype of both A and B:\ncase class C(a: String, b: Int)  \n  extends A with B\nWhich can be passed to f:\nscala&gt; f(C(&quot;Some String&quot;, 42))  \nval res0: String = Some String &amp; 42\nIntersection types are actually the equivalent of compound types, so both A &amp; Band A with B are the same type. However, the with syntax will be deprecated in future versions of Scala.\nUnion Types\nIn Scala, we can express that a value can be either of type A or of type B as Either[A, B]. Consider this example:\nval a = new A { val a = &quot;Some String&quot; }  \nval b = new B { val b = 42 }\n \ntype E = Either[A, B]  \nval l: E = Left(a)  \nval r: E = Right(b)\nThe Either type has two variants: Left, and Right. In the case of E, Left represents the variant that carries a value of type A, while Right carries a value of type B (notice the order of the type arguments passed to Either.)\nLet’s say that we want to define a function g of E. In order to deal safely with values of E, we would need to do some pattern matching:\ndef g(e: E): String = e match {  \n  case Left(a) =&gt; s&quot;String value a: ${a.a}&quot;  \n  case Right(b) =&gt; s&quot;Int value b: ${b.b}&quot;  \n}\nThis is great! We can safely express values that belong to one of two types. But there are a few problems with Either:\n\nIt’s not commutative (Either[A, B] is not Either[B, A])\nIt sucks for working with values that can be of three or more types (I mean, Either[A, Either[B, Either[Float, Boolean]]]? This is a nightmare!)\n\nWhile still thinking of types as sets, let’s think of Either[A, B] as the union of the sets A and B (the set containing all elements of both A and B.) This union can be expressed as A | B:\ntype U = A | B\nUnion types solve the two problems listed above, and they’re actually more pleasant to look at and deal with. Consider the definition of h:\ndef h(v: A | Double | B | Boolean): String = v match {  \n  case a: A =&gt; s&quot;String value a: ${a.a}&quot;  \n  case d: Double =&gt; s&quot;Double value d: $d&quot;  \n  case b: B =&gt; s&quot;Int value b: ${b.b}&quot;  \n  case bool: Boolean =&gt; s&quot;Boolean value bool: $bool&quot;  \n}\nWhich can be applied with simple arguments (no need to wrap in Left or Right.) For example:\nscala&gt; h(true)  \nval res0: String = Boolean value bool: true\n \nscala&gt; h(a)  \nval res1: String = String value a: Some String\n \nscala&gt; h(42.22)  \nval res2: String = Double value d: 42.22\nEnums\nEnums are definitions of types’ values by name. They’re useful when a value of a particular type can be one of a well-defined finite set of elements. For example, the definition of WeekDay:\nenum WeekDay {  \n  case Sunday, Monday, Tuesday, Wednesday,  \n       Thursday, Friday, Saturday  \n}\nEnum values can be parameterized in order to implement algebraic data types (products, to be specific.) In Scala 2.x, we encode ADTs as case classes (or tuples,) and sealed traits. Let’s consider a definition of logical expressions:\nsealed trait VerboseLogicalExpression {  \n  import VerboseLogicalExpression._  \n  def eval: Boolean = this match {  \n    case ConstFactor(c) =&gt; c  \n    ase NotFactor(c) =&gt; !c  \n    case Term(l, Some(r)) =&gt; l.eval &amp;&amp; r.eval  \n    case Term(l, None) =&gt; l.eval  \n    case Expr(l, Some(r)) =&gt; l.eval || r.eval  \n    case Expr(l, None) =&gt; l.eval  \n  }  \n}\n \nobject VerboseLogicalExpression {  \n  sealed trait Factor extends VerboseLogicalExpression  \n  case class ConstFactor(value: Boolean) extends Factor  \n  case class NotFactor(value: Boolean) extends Factor  \n  case class Term(left: Factor, right: Option[Factor])  \n    extends VerboseLogicalExpression  \n  case class Expr(left: Term, right: Option[Term])  \n    extends VerboseLogicalExpression  \n}\nNow we can evaluate the expression:\nscala&gt; import VerboseLogicalExpression._\n \nscala&gt; Expr(Term(ConstFactor(true), Some(NotFactor(false))), Some(Term(ConstFactor(false), None))).eval  \nval res0: Boolean = true\nWhich is just true ∧ ¬false ∨ true. Notice how we’ve had to define a case class for each variant of VerboseLogicalExpression, and made it extend the trait. We also did the same with Factor just to get the behavior of ConstFactor | NotFactor.\nUsing enums, we can define logical expressions as:\nenum LogicalExpression {  \n  case ConstFactor(value: Boolean)  \n  case NotFactor(value: Boolean)  \n  case Term(  \n    left: ConstFactor | NotFactor,  \n    right: Option[ConstFactor | NotFactor]  \n  )  \n  case Expr(left: Term, right: Option[Term])\n \n  def eval: Boolean = this match {  \n    case ConstFactor(c) =&gt; c  \n    case NotFactor(c) =&gt; !c  \n    case Term(l, Some(r)) =&gt; l.eval &amp;&amp; r.eval  \n    case Term(l, None) =&gt; l.eval  \n    case Expr(l, Some(r)) =&gt; l.eval || r.eval  \n    case Expr(l, None) =&gt; l.eval  \n  }  \n}\nMuch cleaner! Note that using the apply method of an enum’s variant would return a value of the enum type, not the specific case type. We can use new to use the constructor of the specific type. So we would define a value of type LogicalExpression.Expr as:\nimport LogicalExpression._\n \nval expr = new Expr(  \n  new Term(new ConstFactor(true), Some(new NotFactor(false))),  \n  Some(new Term(new ConstFactor(false), None))  \n)\nGivens\nGivens are definitions that can be used implicitly. In many ways, they are a more refined version of Scala 2.x’s implicits. Consider this example:\ntrait Add[T] {  \n  def add(x: T, y: T): T  \n}\n \ngiven Add[C] {  \n  def add(x: C, y: C) = C(x.a + y.a, x.b + y.b)  \n}\nHere, we define a given instance of an Add[C]. This is a more straightforward way of implementing typeclasses.\n\nGivens do not need to have names, since their type is all that matters in most cases.\n\nWe can define functions that have given parameters to summon any defined given instance:\ndef zipAdd[T](xs: List[T], ys: List[T])(given a: Add[T]): List[T] =\n\txs.zip(ys).map(a.add)\nscala&gt; zipAdd(List(C(&quot;Welcome &quot;, 18), C(&quot;Scala &quot;, 0)),   \n     |        List(C(&quot;to&quot;, 22), C(&quot;3&quot;, 2)))  \nval res0: List[C] = List(C(Welcome to,40), C(Scala 3,2))\nSimilarly, we can define given value aliases:\ngiven Int = 42\n \ndef addMagicNumber(x: Int)(given magicNumber: Int) = x + magicNumber\nscala&gt; addMagicNumber(2)  \nval res0: Int = 44\nImplicit conversions are replaced with the definition of a given instance of Conversion[T, U]. For example, we can define an implicit conversion from String to A as:\ngiven Conversion[String, A] {  \n  def apply(s: String) = new A { val a = s }  \n}\nscala&gt; h(a) // a is converted from A to String  \nval res0: String = String value a: Some String\nscala&gt; :type h  \nA | Double | B | Boolean =&gt; String\nUsing wildcard syntax in imports will not import any given definitions. Instead, you must use .given syntax, or import the given definitions by name. See here for more information about imports.\nExtension Methods\nScala 3 offers a really simple way to add new methods to existing types. Consider the following example:\ndef (x: T) +[T] (y: T)(given a: Add[T]) = a.add(x, y)\nThis definition (when in scope,) adds the + method to any type T for which a given instance of Add[T] is defined, such as the type C.\nscala&gt; C(&quot;Yay &quot;, 2) + C(&quot;Simplicity!&quot;, 40)  \nval res0: C = C(Yay Simplicity!,42)\nIn 2.x, you would have to define an implicit class with the extension method defined on it to achieve this. This is a much simpler approach.\nConclusion\nThere still are many features not covered here: typeclass derivation, match types, and an entirely new macro system, just to name a few. This article would need to be much longer than this in order to cover all of them. Luckily, the documentation offers a great introduction to these concepts, even though it might be lacking in some regards, but that’ll surely get better with the official release of Scala 3."},"Blog/DevOps---The-Simple-Version":{"title":"DevOps - The Simple Version","links":["Bits/Do-Not-Re-Invent-the-Wheel"],"tags":[],"content":"DevOps has gotten to the point where it’s not even easy to define. The field has a lot of history, and with it comes scary terms like “Continuous Integration”, “Container Orchestration”, and worse. I was completely lost when I dove into the subject a few years ago, so I thought I’d make it easier for those after me to wrap their head around what DevOps engineers do, and how.\nThis post gives an overview of DevOps, its common practices, problems it solves, and the most popular tools used to solve these problems. Whether you’re a fresh grad pursuing a career in DevOps, a hacker looking for the right tool, or an expert in need of a refresher, this post is for you.\nDevelopment\nAny software engineer (I hope) should be able to write code, run it, version it with Git, and ultimately submit it for production release to be available to end-users.\nAfter the code is released, a bug is reported. It gets assigned to a hapless engineer who fixes it  by writing code, testing it, committing it to Git, and opening a pull request to be merged and released. The engineer is then free to start working on a new feature, which involves writing code, running it, pushing the changes, and opening a PR. This is the development cycle.\n\nThis is what software developers do: write code to produce software applications that automate business. However, it turns out that developers also need to automate their own work as well.\nMany software production tasks must be automated in order to shorten the time it takes to complete the development cycle. Not only to maintain the sanity of developers, but also to maximize their productivity. The faster working code gets shipped to end-users, the better.\nOne such process that should be automated is building executable programs, which you, as a developer, do multiple times every day. A build tool is the program that compiles all your code files and external libraries and glues them all together to produce an executable  application, ready for you to run during development, or send it to a user.\nNow, imagine that every time you wanted to run your code after editing it, you had to manually compile every file or module, and then glue the results together to form an executable program. And of course, that wouldn’t work the first few hundred times because you’ve messed up the order in which you compiled your modules, or you tried to skip files you’ve already compiled only to realize that you’ve modified one of their dependencies and you actually needed to re-compile the code with the new dependencies.\nWithin a week, you would either go insane, or get fired for getting nothing done.\nThankfully, developers before us have already walked this minefield and cleared the path for us. They created build tools like Maven for Java, or Webpack for JavaScript. They automated the process of correctly compiling and gluing things together to produce a runnable app.\n\nFast-forward a few decades and you’ll find that developers didn’t stop at automating builds, but kept automating other tedious and error-prone tasks. However, these tasks significantly grew in complexity with the growing demand for more scalable systems and processes.\nBusinesses now have more users. Users now demand more, so businesses hire more developers.  Developers need to keep up with the increasing needs of users, so the needs of developers increase as well. More servers, more storage, more complex software architectures, and all of it needs to be reliable, fast, cost-effective, and secure.\nEven with all this added complexity, developers are still expected to deliver quickly. Developers needed to specialize. Some should handle the needs of users, while some should handle the needs of their fellow developers.\nEnter the DevOps engineer: the engineer whose job it is to ensure that operations run smoothly. And I know “operations” and “smoothly” are broad terms, so hang on.\nCI/CD\nSo, you’ve built your app using your fancy build tool, you ran and tested it, pushed your changes, and opened a PR. Now what? Well, in some companies, this is where you would wait for your changes to be approved by someone else. When you get a green light, you merge and deploy the code, i.e. you run a script or click a few buttons to upload your program to a server somewhere, where it’ll be accessible to end-users.\n\nBetter yet, in some companies, you wouldn’t even need to run that script or click any buttons; instead, your code is shipped to end-users automatically when it is approved and merged. This automation is called Continuous Delivery, or CD for short.\nSome also take the opportunity to run automated tests automatically when code is pushed, so that the reviewer (the one to approve the changes) would see that the new code mostly works and doesn’t break any existing code. It’s also nice to keep checking changes against upstream code to make sure merge conflicts are resolved as soon as possible. This automation is called Continuous Integration, or CI.\n\nSome take automation even further and deploy a preview environment for every pull request opened. The reviewer (or otherwise, any stakeholder) can go see the new code in action, without having to build the app themselves to run it on their machines. They just go to a link where the new version of the app is hosted, and play with it until they’re satisfied with the work. Once the PR is merged, the preview environment is safe to delete entirely in order to save costs.\nIn such companies where all this automation is implemented, it’s likely that you’ll find good DevOps engineers. A DevOps engineer’s job is to help developers focus on writing code. Part of the job is to completely automate the deployment workflow. The developer isn’t really required to learn how to get code from the Git repo into the hands of end-users.\nIt’s enough that a developer knows how to correctly implement business requirements. Developers focus on user-facing features, while DevOps focus on developer-facing features. Developers don’t need to waste time deploying code to test environments for stakeholders.\nTools\nCI/CD workflows can get complicated very quickly. You need to trigger them on specific events, like merging a PR into the main branch. Probably, you’ll need to talk to multiple cloud services and SaaS to get the app deployed. Most tasks you’ll perform take a long time to run, and some of them depend on one another to complete. Some tasks can be performed in parallel to speed up the workflow. Some tasks are asynchronous, but you still need to make sure they’ve completed successfully.\n\n\nWe’ll talk about Docker in a bit.\n\nThe worst thing you can do when setting out to automate your CI/CD is to create an empty file and start writing a bash script. Please, don’t re-invent the wheel. Most people use workflow orchestration tools to solve these problems.\nMany workflow orchestration tools let you specify your workflows in YAML files, and many have CI/CD-specific features. Simple, yet extremely valuable forms of CI/CD can be achieved for free using GitHub Actions. There are also open source solutions like Jenkins or GitLab that you can self-host. And of course, giant cloud providers offer their own tools, like Google’s Cloud Build, AWS CodePipeline, or Azure Build.\nInfrastructure\nYour server-side apps need to run on servers - actual machines with CPUs and memory. You also need databases, file storage for uploads, and probably much more. Most people use the cloud these days. Providers like Google, Amazon, Microsoft and many others let you pay them in exchange for allowing you to use their computing resources.\nHowever, the services that these clouds provide do not stop at lending you bare metal machines. It’s too much work for you to manage a bunch of machines directly when all you want is to deploy an application with a database and some file storage.  Instead, they provide managed services that take a lot of load off your back, especially if you barely know what you’re doing.\nFor example, you can use a managed database service that automatically takes care of data backups and encryption, and increasing disk sizes according to your usage, without you having to worry about running out of space. A managed database service might also take care of scaling the compute and memory of your database according to load.\nYou should be careful when using managed services though. Some of them still require considerable knowledge to use effectively and efficiently, since these services automatically increase the resources you’ll end up paying for. Read the docs and understand the pricing very well.\nTools\nThe list of cloud computing platforms is endless, but there’s a few really big ones:\n\nGoogle Cloud Platform (GCP)\nAmazon Web Services (AWS)\nMicrosoft Azure\n\nThese provide everything you’ll need to build almost anything, no matter how large or small. However, capitalism has its way, and many smaller cloud providers compete with the giants:\n\nDigitalOcean\nCloudflare\nHetzner\nLinode\nVercel\n\nAnd many, many others. Each has its own strengths and weaknesses. Pricing is a factor, but the services each provider offers could be the primary reason to pick one provider over the other.\nThe big providers have the advantage of offering a comprehensive, cohesive suite of products and services that integrate well together, while smaller providers compete with lower prices (Hetzner and DigitalOcean) or incredible developer experience (Vercel). In the end, you’ll need to study what each provider offers to best fit your needs.\nCloud providers usually have a Web interface that you can use to ask for their services. You’ll think I’m crazy at first, but I think you should not use the UI to provision your cloud infrastructure.\nI do agree that it’s kind of magical to simply log into your cloud provider’s UI, click a few buttons, and now you have a managed database up and running. It’s quite convenient, sure. But:\n\nWhat if you end up using multiple cloud providers?\nWhat if you end up having to provision lots of databases? Do you wish to click through the same wizard a hundred times?\nWhat if you click around and everything falls to pieces? How would you know what you did in order to undo it? Once you do know what you did wrong, is it easy to undo?\nWhat if the UI is AWS crap?\nWhat if you need to replicate your production infrastructure for staging? How do you keep them in sync?\nWhat if someone else needs to review the changes you want to make to the infrastructure?\n\nAll the above problems can be solved if you define all your infrastructure as code. You write files that describe what you need from the cloud provider, and you let a tool talk to the cloud provider and see what it needs to do to make what you’ve specified in the code match what is actually provisioned.\nThe most popular and well-supported Infrastructure as Code tool is Terraform. An open-source drop-in replacement to Terraform is OpenTofu.\nDeployment\nSo how does your code get from the Git repository to the server where it runs? How and when does it start running? How do you make sure it will restart if it crashes? How do you make sure there’s no downtime when deploying new versions of your app? Let’s answer these questions one by one.\nContainers\nIf you were thinking of SSH-ing into your server and cloning your repo, or uploading your code to the server via FTP, don’t. There are much better ways these days. You’ve likely heard about Docker by now, but how does it help us get our code to the server?\nDocker is known as a container runtime, and it also provides tools for building, storing, and distributing container images. Practically, a container is a process that can run on any machine as-is. A container image is a template that describes a container. You can use an image to create as many containers as you wish, similarly to how you can have a class and create multiple instances from it in object-oriented programming.\nWhat’s nice about using Docker for deployments is that a Docker image contains everything your code needs to run, so you only need your server to have Docker installed, and then you can run any Docker image you want without installing anything else. For example, if you have a NodeJS app, you build an image that contains both the NodeJS runtime and your application code that runs on top of the NodeJS runtime..\nWhen you finally have an image for your app, you can upload it to one of many container registries, which are services that store your images. Once the image is stored in a registry, you can easily pull it to your server and run it.\nDocker has become the industry standard for packaging server-side apps. You should really get comfortable with it. Not only to package your own apps, but also to more easily run other people’s apps, be they apps built by other teams in your company, or open-source apps built by the developer community.\nTools\nYou can upload your Docker images to one of these registries:\n\nDockerHub\nGitHub Packages\nGoogle Artifact Registry\nAWS ECR\nAzure Container Registry\n\nContainer Orchestration\nYou now have an easy way to get your apps to servers, but how do you run them? The naive way would be to SSH into the server and run docker run to start your app, and when there’s a new version of your app, you SSH into the server again, kill the old container, and run the new image.\nThis way of doing things is simple, sure. But it has its issues:\n\nIt introduces downtime when you kill the old container, and if the new container fails to start, there’s even more downtime until you get the old one running again\nIt becomes unmanageable if your app runs on multiple servers (more on this shortly)\n\nAnd to complicate things further, you shouldn’t even run your apps directly with docker run for many reasons, including the fact that Docker on its own is not enough to ensure that your app stays alive - it has no notion of health checks, and it’s not enough for managing a distributed application (an app that runs on multiple servers).\nNow here’s where you (the DevOps engineer) need to make a choice: can the business tolerate having a bit of downtime on every new deployment? Does the business not have enough traffic to warrant having multiple servers to handle the load? If the answer is yes, you can probably get away with a very simple deployment setup. But it’s too often the case that the needs of the business require elaborate setups.\nTools\nIn order to satisfy your business’s availability and scalability requirements when it comes to managing deployments, you’ll likely find yourself reaching for a container orchestration tool. These tools manage your containers for you.\nFor instance, there are tools that keep probing your app to see if it’s still healthy and responding as expected. If not, the tool kills the app, runs a new container, and alerts you to the problem. The same tool can also manage multiple instances of your app running on multiple servers, which can ensure that there’s always at least one server ready to serve requests at all times if all other servers crash. This tool would also help you with gradual rollouts, meaning that when you have a new version of your app, the tool stops sending traffic to the old version of the app only after it makes sure that the new version is up and running, ensuring zero downtime on each deployment.\nThe simplest, but possibly the most costly solution for container orchestration is to use a cloud service that simply takes your Docker images, and runs them. It’s that easy. You don’t need to know anything about the actual server where your container runs. These services are known as CaaS, or Containers as a Service. Because you don’t need to know anything about servers to use these services, they’re often called serverless. Most of these services have a decent free tier.\nHere are some managed cloud services that can run your containers:\n\nFly.io\nRender\nCloudflare\nHeroku\nDigitalOcean Docker Hosting\n\nAnd of course, the giants have their own solutions:\n\nGoogle Cloud Run\nAWS AppRunner\nAzure Container Apps\n\n\nSome serverless hosting services can host your applications without even requiring you to package your app in a Docker image, such as AWS Lambda, Google Cloud Functions, Azure Functions, Cloudflare Workers, and Vercel. However, support for programming languages and frameworks is limited. With containers, you have the freedom to use any language or framework, at the cost of having to build the Docker image yourself.\n\nThere also exist many open source solutions out there, and you’ve probably heard of them by now. Kubernetes (k8s for short) is currently the most popular container orchestration framework, with a huge, vibrant ecosystem of tools built around it. With that said, you probably shouldn’t be using Kubernetes on its own, but rather leverage the tools built on top of it.\n\nIf you’ve never deployed an app yourself before, don’t use Kubernetes. Stick to managed cloud services instead.\n\nHere are some tools and frameworks built on top of Kubernetes that can dramatically simplify deployments:\n\nKnative - the framework that Google Cloud Run uses under the hood\nOpenFunction for serverless functions\nKubeflow for machine learning workflows\n\nKubernetes, and all projects mentioned above are in some way part of the Cloud Native Computing Foundation (CNCF). Any CNCF project is worth checking out, since these projects are sure to be open-source, well documented, and well maintained. These projects are also likely to be foundational and extremely valuable.\nThe term “Cloud Native” likely means that the thing works very well with Kubernetes, since Kubernetes is synonymous with open cloud computing these days. This is how important and foundational Kubernetes is.\nTo make working with Kubernetes more familiar for developers, there are many tools that help you manage Kubernetes using Git, like you would manage any other code. These tools follow GitOps principles, which basically aim to apply software engineering best practices to DevOps. The two most popular GitOps tools are Argo CD and Flux.\nMonitoring\nWhen your app is deployed and running in production, you aught to keep an eye on it to see how it’s doing. Is it throwing errors in response to specific user inputs? Is it consuming too much memory? Is it still fast? And if my app runs on multiple servers, how do I trace requests between them?\nThe first thing we all learn as programmers is how to print “Hello World” to standard output. We also often use prints to debug our applications during development. Printing is sometimes very useful in production as well, error messages and stack traces in particular. Logging things to standard output (or files) is one of the simplest ways for us to observe what happened inside our application.\nIn a production environment, logs should be stored somewhere for us to revisit when something goes wrong, or to extract insights about our app. So it’s useful to think about logs as data that should be stored in a database.\nLogs are great, and everyone should use them, but they’re not great for everything. They won’t help you if you’re trying to know how much memory your app is using. Many argue logs aren’t even great for measuring any type of metric, even though it’s possible. You can measure the number of times an HTTP endpoint was called by counting the number of times a particular log appeared, but you might soon find that things got complicated.\nIn any case, if logs alone serve your needs well, that’s good. But it’s often the case that you need more precise types of data.\nThere are three common types of data that helps you know what’s going on with your deployed applications: logs, metrics, and traces. These three types of data are often called telemetry data, and the process of making your apps produce all this data is called instrumentation.\nLogs and metrics are widely known, but what are traces? In short, traces are similar to regular stack traces in programming, but they can be distributed across different machines and applications. You can also store useful data in traces to ease debugging, or to make certain traces easy to find.\nTools\nSo how do you collect, store, and access all your telemetry data? The good news is that you’re well covered by existing tools. The bad news is that you might need to set it all up as a DevOps engineer.\nIf you’re using a managed cloud service to host your app, you probably don’t need to do anything to get the essentials. Most serverless hosting services store your logs and let you query them, and they also give you basic metrics for HTTP requests and resource utilization.\nEven if you’re not using a managed service to host your application, you can still use managed cloud services to monitor your application. New Relic and Grafana Cloud are pretty popular APM (Application Performance Monitoring) platforms. There are also open-source APMs like:\n\nSentry\nSigNoz\nHighlight.io\n HyperDX\n\nYou’re also covered with the giants:\n\nGoogle Cloud Monitoring and Logging\nAWS CloudWatch\nAzure Monitor\n\nHowever, shipping telemetry data to managed service could be annoying to set up. You may need to run and manage an agent that collects data from your applications and machines, and ships it to the telemetry service to be stored. You also need to make sure your app exports telemetry data in the format that the service provider expects.\nOpenTelemetry is a CNCF project that standardizes telemetry data formats and transfer protocols, and it’s widely supported by cloud service providers. The best way to instrument your applications is to make them export OpenTelemetry data using the OpenTelemetry SDK for your languages. This helps you avoid changing code if you ever need to change your telemetry service provider, and saves you a lot of time in the future when you want to integrate new applications with new providers, since you’ve already learned how to use OpenTelemetry.\nIf you’re planning on hosting your own application observability infrastructure, there are many open source solutions available. The open-source solutions mentioned above are all-in-one solutions that you can self-host, but you can get more granular depending on your needs. Be warned though, managing these things in production is risky if you don’t know what your’re doing:\n\nElastic Stack or Grafana Loki for collecting logs\nPrometheus or Victoria Metrics for collecting metrics\nGrafana Tempo or Jaeger for collecting traces\nGrafana for querying and visualizing telemetry data from any of the above sources\n\nHosting any of these solutions on Kubernetes is made much easier for you with the help of operators. However, there are solutions specific to Kubernetes that can make life much simpler for you with fully automatic instrumentation, such as Pixie.\nConclusion\nWe’ve covered a lot in this post, so don’t feel like you need to understand all of it right away. Keep this post as a reference for the future. But if there’s one thing I’d like you to take away, it’d be that you should take the time to study Kubernetes and its ecosystem if you’re considering a career in DevOps.\nIt’s also important to remember that we do what we do to satisfy business needs, so keep things as simple as possible."},"Blog/FeathersJS-For-The-Beginner---Basic-concepts":{"title":"FeathersJS For The Beginner - Basic concepts","links":["Blog/FeathersJS-For-The-Beginner---Hello-World!"],"tags":[],"content":"\nFeathers is great, it enables you to get a huge amount of work done in no time, it performs nicely, and it’s elegantly designed. But it’s not so beginner friendly, and by beginner, I mean someone who knows some JavaScript, played a little bit with Node.js, and knows very basic things about the internet, i.e. what HTTP is, and client-server architecture.\nThis article covers the very basic things a beginner needs to understand about FeathersJS in order to hopefully make learning from other online resources much easier.\nFeathers is a JavaScript framework that is used for building RESTful, and real-time web APIs, let me explain:\n\nAn API is simply software that allows programs to communicate with each other.\nA RESTful API is an API that takes advantage of HTTP methodologies for resource management, where the HTTP GET request method is used to retrieve resources, POST is used to create new resources, PUT is used to update existing resources, and DELETE is used to remove resources.\nA resource can be any item of value, such as a JavaScript object, or a text file.\nA real-time API is an API that allows for very low-latency communication between the server and the client, where clients can receive updates whenever they’re published by the server, instead of constantly requesting the updates, such functionality can be easily achieved using WebSockets.\n\nSince Feathers is a Node.js framework, it uses many well known and documented technologies under the hood, e.g. Express.js for the RESTful part of the application, and Socket.io for WebSocket communication.\nThe Feathers Architecture\nFeathers is made up of very simple building blocks, at the heart of which is the app object, which represents the entire application, and connects all app components together. We’ll discuss the app object in more detail later.\nServices\nA service is the main ingredient of any feature your application implements, it contains the core functionality behind anything your users might want to do with your application, for example, a single service might be responsible for one of:\n\nAuthenticating users.\nSending SMS verification codes to new users.\nPerforming CRUD operations on a certain entity in your database, such as users, or posts.\n\nHere’s what a service looks like:\n\nAt a low level, a Feathers service is a JavaScript object that implements one or more of the following methods:\n\nget: Retrieves a specific resource by its unique identifier, for example get the order with the id 5.\n_find: R_etrieves multiple resources, for example, find all posts where the user id is equal to 29.\nupdate: Replaces a resource’s value, for example, update the user with id 645 to be equal to the value: \n{id: 645, email: &quot;johndoe@gmail.com&quot;, password: &quot;ihatepasswords123&quot;}\ncreate: Creates a new resource, for example, create a new post with the value: {id: 3293, user_id: 645, text: &quot;My first post!!! yay.&quot;}\npatch: Merges data with the existing resource, for example, patch the user with id 645 with {email: &quot;johndoe123@yahoo.com&quot;} (this will change the user’s email to johndoe123@yahoo.com)\nremove: Removes the selected resource, for example, remove the user with id 645.\n\nNote: The examples above use JSON as the resource type.\nHer’s what a user service might look like:\n\nIf you would like to read ahead of this article, visit the services API documentation.\nHooks\nA hook is a function that you can attach to your services to customize them, for example, you can add a hook onto the users service to validate the email of every user before they are created.\nHooks are very powerful and easy to use, they give you a way to easily filter and transform service input and output data.\nHere’s what a users service might look like with hooks:\n\nLet’s walk through what each of these hooks can be doing:\n\nAuthenticate: Authenticates every user before allowing them to perform any action.\nValidate email: Processes the input email to make sure that it conforms to a certain format.\nSend confirmation email: Sends an email to the user’s address for them to confirm that is was indeed the email’s owner who signed up.\nProtect password: Removes the password from any result going out to a client.\n\nWhen defining a hook, you can specify when the hook is executed, and with which service methods, for example, you want to authenticate the user for every service method call, while you may want to validate the user’s email only on create, patch, and update method calls.\nI will cover hooks in more detail in future posts, but in the meantime, feel free to refer to the hooks API documentation for more details.\nEvents\nFeathers implements a very useful event mechanism, where a service can publish an event, and everyone listening to that event will receive real-time updates.\nBy default, a service will automatically fire events for each successful call of any of its methods, so for example, you can listen to the “message created” event and receive updates whenever a new message is created.\nEvents are essentially what enable Feathers’ real-time functionality, and they sometimes allow for very simple solutions to problems that may seam complex at a glance.\nYou can read more about events here.\nChannels\nWhen publishing events, you normally don’t want to send the event to every connected user, it is better to send the client only the messages that they need.\nYou can think of a Feathers channel as a chatroom, where multiple users join and only receive the messages sent to that room.\nI’ll cover channels in more detail in a future post, where we’ll build a real-time Socket.io API, but you can read about them here if you just can’t wait.\nConclusion\nThis article isn’t meant to teach you how to use Feathers, it simply walks you through some of the very basics that might’ve been ignored in other existing learning resources, hopefully it made your life a little bit easier.\nYou can check out part 2 of this series, where we build a fun little Hello World application."},"Blog/FeathersJS-For-The-Beginner---Hello-World!":{"title":"FeathersJS For The Beginner - Hello World!","links":["Blog/FeathersJS-For-The-Beginner---Basic-concepts"],"tags":[],"content":"In my last post, I explained the fundamental concepts that one needs to understand in order to leverage the power of Feathers, but in this post, rather than having a high-level overview of how the framework works, we’ll be creating a very simple Feathers application that can say “Hello” to the world.\nThis app will consist of a single service that says hello to anyone who writes their name in the commandline, and it remembers their names. It can also say hello to everyone it knows if asked to.\nBefore we start building this amazing application, make sure Node.js is installed on your machine, if not, you can download it from here. I’m using Node version 8.11.2 and npm version 5.6.0 for this app.\nLet’s get started by creating a directory named feathers-hello-world to put our project in. From your terminal, create the directory and change your working directory to it, like so:\nmkdir feathers-hello-world  \ncd feathers-hello-world\nFrom here we can initialize our project with :\nnpm init -y\nWhich will create a package.json file, which contains details about our project, including declarations of the project’s dependencies.\nNow let’s install our only dependency, @feathersjs/feathers:\nnpm install --save @feathersjs/feathers\nWhen all is done, your package.json file should look like this:\nNow let’s create an index.js file to write all of our code in:\ntouch index.js\nNow let’s initialize our application inside index.js like so:\nIn the first line, we import FeathersJS in order to initialize our _app_ object using it in the second line.\nNow let us implement our HelloService, which implements only two service methods: create, and find.\nLet me explain what’s going on:\n\nIn the constructor, we initialize the names array, which is the array that we’ll store people’s names in.\nThe create method is one of the service methods, and it is responsible for creating new records of people we may want to greet again in the future.\nThe create method accepts two parameters: data, which is an object containing the data we will be storing, and params, an optional parameter that can contain extra data we can use in the process of creating the record.\nNot only does the create method store the name of the new visitor, but it also returns a personal greeting for that person.\n\nNote: A service method must return a JavaScript Promise, or it must be an asynchronous function.\n\nThe find method is another one of the Feathers service methods, it is responsible for retrieving all greetings the application had ever greeted.\nThe find method accepts one parameter: params, which can contain information that can be used in searching the records for more specific results.\nThe helloTo method is simply a helper method that is used by both service methods to turn people’s names into greetings.\nIn the first part of this series, I briefly mentioned the app object, and how it ties our application together, and in the last line we see how a service is registered using the use() method of the app object. Every service in our application has to be registered in the same manner.\nThe first argument of app.use() is the path to the service, which is the string that you’ll be using to get a hold of the service through the app.service() method. The second argument is a new instance of the service you want to register, or simply an object that implements one or more of the service methods.\n\nOur service is now ready for use. Now let us write a little bit of code to allow the user to interact with the app through the commandline:\nNotice how we used app.service(&#039;hello&#039;) to obtain a reference to the HelloService , this is the method you should use to reach any service in your application.\nNow if you run the app with the command:\nnode index.js\nThe app will say hello to you if you type in your name and hit Enter, but not just you, it can say hello to anyone who tells it their name!, friends, family, really anyone you introduce it to!, and if you want it to say hello to everyone it knows, you can type “everyone” and hit Enter, how cool is that?…… not so cool I’m afraid. This is how index.js should look like now:\nWhat is cool though, is that you can turn this pp into a RESTful API with one command and 6 lines of code. Start by installing @feathersjs/express :\nnpm install --save @feathersjs/express\nNow change your index.js file to look like this:\nI’ll spare you the details of RESTful Feathers APIs until my next post, but for now, it suffices to say that calling service methods using HTTP is very similar to the way we called them locally, where you would use HTTP GET to call the find method, and POST to call the create method.\nWhen running node index.js , you should see the following printed out to your terminal:\nHello World app is running on http://localhost:3000\nUsing a tool like CURL or Postman, you can send an HTTP POST request to http://localhost:3000/hello withe the JSON body: { &quot;name&quot;: &quot;John Doe&quot; } and the server would respond with a friendly “Hello, John Doe!”. If you send an HTTP GET request to the same URL, you’ll get an array of greetings for all the names that you’ve posted to the hello service.\nConclusion\nIn this post, we’ve built a very basic Feathers application that consists of a single service, which allowed us to see two of the service methods in action, and we saw how easy it is to use said service using REST. In the next part, we’ll be building a more practical, persistent RESTful API, which will give us the chance to look at how hooks are implemented, and how easy Feathers makes accessing your database with database adapters."},"Blog/Functional-Programming---The-Simple-Version":{"title":"Functional Programming - The Simple Version","links":[],"tags":[],"content":"\nI think functional programming (FP) has become a necessary skill to have for any software developer or programmer in general. Considering the tools used in industry today, like React.js, Elm, and all the popular functional languages, you don’t want to miss out on all the awesomeness. Despite the fact that you need a certain level of understanding of functional programming in order to be proficient in use of these tools, understanding of the general concepts of FP can make you a much better software developer in general. Plus, a lot of frameworks and programming languages that aren’t considered functional actually have functional features. Python, JavaScript, Java, C#, Dart, and many other languages have quality of life functional features that can save you a lot of time and headache if you know how to use them properly.\nIf you look carefully at these tools, you notice a lot of underlying concepts that are common to all of them. Having a solid understanding of the fundamentals on which these tools were built can carry you much of the way towards mastering them, and certainly makes your life easier when learning to use new tools (and judging them.)\nIn this article, I attempt to explain the basics of FP using Haskell. Don’t worry if that sounds scary, it’s really simple stuff.\nFunctions!\nIf you squint hard at any computer program, you notice that it’s essentially the following:\n\nIt’s a process that you feed data, and outputs something. The input can be anything from user clicks, to command line arguments, and the output can be a file, or search results.\nThere’s a really simple way to express this notion of transforming input to output: mathematical functions. If you already have a good understanding of functions, I can understand why you might be skeptical of the idea that any program can be modeled as a mathematical function. You’re right, a function can’t be used to model writing a file to disk. However, functions are the simplest way to model data transformations, which make up most of our programs..\nYou may have learned about functions in programming. When I’m talking about functions, I do not mean “customizable and reusable pieces of code.” What I mean is way simpler than to print something to the screen, or to change the value of some variable outside of it. If you’re not familiar with mathematical functions, let me explain.\nA function in mathematics is a relation between two sets A and B, such that every element in A is associated with one and only one element of B. Sounds complicated? This might help:\n\nIn essence, a set is a collection of unique elements. In this example, we have two sets: Names (with a capital N,) and Lengths (with a capital L.) These two sets are related in the sense that every name has a length (the number of characters in it.) Since every name is associated with only one length, we can call this relation a mathematical function. Let’s call this function length.\nFunctions can be thought of as mappings between elements of sets. In length, the element “John” is mapped to 4. We note this mapping as length(“John”)=4, which in plain English translates to: length of “John” equals 4. We can also say that applying length to “John” yields 4.\nA value passed to a function as input is called an argument of the function. When a function is applied to an argument, the result is called the image of that argument, and the argument itself is called the preimage . In length, the image of “Jane” is 4.\nNotice how the mapping has a direction: it goes from Names to Lengths. More generally, we call the “original” set the domain of the function, and the set to which the elements of the domain are mapped the co-domain. For length, the domain is the set Names, and the co-domain is the set Lengths.\nYou can type the expression length(&quot;John&quot;) into this Haskell REPL to the right and hit Enter to get the result. A REPL is just a program that reads, evaluates, and prints the result of expressions you give it. Feel free to try running any code you see in this article.\nAny function can be expressed as a set of pairs. Each pair in this set consists of a value from the domain, and its corresponding image in the co-domain. Our little length function can be expressed as:\n[(&quot;Jane&quot;, 4), (&quot;John&quot;, 4), (&quot;Jackson&quot;, 7)]\nwhere all the elements between the brackets ([]) form a set. In programming (inconveniently,) it is not necessarily the case that all the pairs that form a function are known before executing it. Imagine a function from the infinite set of integers (ℤ) to the infinite set of real numbers (ℝ.) How can we possibly fit all the pairs that form this function in the finite memory of a computer? Stay tuned.\n\nI guess it’s appropriate now that I define functional programming. FP is using mathematical functions as the primary building block of programs. In other programming paradigms, the primary building block would be instructions, or objects. Using functions instead gives you some superpowers that are beyond the scope of this article, but they’re very simple, which is a huge plus.\nEquations\nEarlier I implied that in programming, we normally don’t know all the pairs of preimages and images that form a function. Most of the time, we can’t even write these pairs down because there are so many (possibly infinitely many.) Consider a function that maps an integer to its increment (the number + 1.) This function is impossible to write out as a set of pairs because the integers never end.\n\nIn such cases, we can express the function as an equation. A formula that the computer can use to compute the image of a given argument. In the case of our increment function, writing the equation isn’t very difficult:\nincrement(x) = x + 1\nThis equation is read: “the increment of x is equal to x + 1.” Isn’t it wonderful! Instead of writing out the mapping with concrete arguments in this form:\nincrement(1) = 2  \nincrement(2) = 3  \nincrement(3) = 4  \n...\nWe can generalize the formula to work with any given integer by making the argument’s value unknown in the equation, making it a parameter. The image of a particular value can be computed by applying the function:\nincrement(60)\nThis expression will yield 61 (the image of 60.)\nIn Haskell, you actually don’t need parentheses in function definitions and applications. You can use spaces instead:\nincrement x = x + 1  \nincrement 60\nWe can call function definitions equations in Haskell because the two sides are literally equal. You can take out any application of a function and put the evaluated right-hand side of the function equation in its place, and you would get exactly the same result. The same can be said for any Haskell expression. Think of it as being able to take out 250 * 2 and placing 500 instead of it. This property is known as referential transparency, and it just makes programs much easier to think about_._\nFunction Types\nRemember what I said about mapping having a direction? Let’s take a closer look at our increment function. It is a function that maps the elements of the set of integers (ℤ) to the set of integers + 1 (also ℤ.) In mathematics, this direction is noted as ℤ ↦ ℤ. In Haskell, we think a lot about the types (basically sets)of data that we operate on. There are many predefined sets already built into the language, such as Int for integers, Float for floating-point numbers, Bool for True and False, and String for text. But functions have types of their own. A function going from integers to integers has the type Int -&gt; Int.\nWhen defining a function, it is almost never necessary to explicitly specify its type. The Haskell compiler can figure the type of the function on its own by looking at how the function’s parameters are being used. Adding type annotations makes it easier for you to just look at the function’s name and type (its signature) and immediately have a good idea of what it does.\nThe definition of increment with explicit type annotations is as follows:\nincrement :: Int -&gt; Int; increment x = x + 1\nNow you can read this definition as: “increment is a function from Int to Int; the increment of an integer x equals x + 1.”\nWhen writing Haskell code in a file (not in the REPL,) you can write the type annotation on one line, and the actual function definition on the next line. This makes it possible to omit the semicolon (;.)\nLooking at our definition of increment with the eyes of a lazy programmer, I can see a small problem: it can only be used to increment integers. Wouldn’t it be great if the function could be applied to arguments of any numeric type? It would make sense for us to use the same function to increment floating-point numbers too, right? Coming soon.\nFunctions of Multiple Parameters\nLet’s say we want to define a function f that takes two integer parameters: x and y. The function squares x and adds it to y. This is how it would look in Haskell:\nf x y = x * x + y\nSimple enough. Now you can apply f to two space-separated arguments:\nf 2 3\nwhich yields 7. Great! Now on to the weird part.\nIf we were to redefine f with type annotations, it would look like this:\nf :: Int -&gt; Int -&gt; Int; f x y = x * x + y\nThat’s strange… Look at the type of f… It’s Int -&gt; Int -&gt; Int! Why is that? Shouldn’t it be (Int, Int) -&gt; Int since it takes a pair of integers and maps them to a single integer? And what does it mean to have two arrows in a function’s type anyway?\nThis weirdness is a consequence of a concept called currying, which I’ll explain when it becomes useful.\n\nLet’s take a break from types for now. In order for the rest of the concepts I’d like to explain to make sense and feel useful, I feel it is appropriate to take a look at a very important data structure first: the list.\nLists\nWhen you have a number of related data points that you would like to process together, it’s a good idea to store them in some sort of container that preserves their structure, and gives you some tools to help with your processing task. Lists are great data structures that do just that. They can be used to store data in an ordered manner, in the sense that the first element in the list comes before the second element, and the second comes before the third, and so on. Lists also let you easily walk through all the elements within them, transform them, filter them, and aggregate them.\nLet’s define a list of the integers from 1 to 5 and name it oneToFive:\noneToFive :: [Int]; oneToFive = [1,2,3,4,5]\nYou’ll notice that the type of oneToFive is [Int] (read “list of Int.”) We can construct lists by using square brackets ([]) to surround the comma-separated elements of the list. This is awesome! Let’s see what we can do with this list.\nIn Haskell, there are lots of predefined functions that operate on lists. One of these functions is head, which maps a list to its first element. For instance:\nhead oneToFive\nwould yield 1 (the first element of [1,2,3,4,5].) Another function is tail, which maps a list to its self, but without the head:\ntail oneToFive\nwill yield [2,3,4,5](oneToFive without 1, the first element.) A list can be pictured as a snake, even though this is not a perfect analogy — snakes can’t regrow their heads:\n\nTake the list’s head, and you’re left with its tail. A list’s tail is a list itself, so it has a head and a tail. Take the list’s tail’s head, and you have the second element (2.) Take the list’s tail’s tail’s tail’s tail’s head, and you’ll get 5:\nhead(tail(tail(tail(tail(oneToFive)))))\nHow fun. What about the head of an empty list? Well, it doesn’t exist, so you can’t get it. The same goes for the tail of an empty list. Now, let us think for a moment about the tail of a list with one element. That’s tricky, but let’s reason about it this way:\nIf we have a list of five elements, then its tail should have 4 elements (5 minus the head.) So we can say that the length of the tail is the length of the whole list minus one. But it doesn’t make sense for a list to have a length less than 0 (being empty,) so the base case, or the most basic case for a list, is to be empty. So it makes sense for the smallest possible tail to be the empty list.\nOther than using square brackets to construct lists, you can use the colon operator (:, a.k.a. the construction operator) to stick a head to a tail, therefore constructing a new list. For example, oneToFive can be defined as:\noneToFive = 1:2:3:4:5:[]\nThe expression is evaluated from right to left. We’re sticking 5 to [], 4 to [5], 3 to [4,5], 2 to [3,4,5], and 1 to [2,3,4,5], thus getting [1,2,3,4,5]. Using this notation, it’s clear how the smallest possible tail of any list is the empty list ([].) Without it, you wouldn’t be able to construct any list, because you wouldn’t be able to “end the cycle.” This brings us to a very important concept in functional programming: recursion.\nRecursion\nA list can either be empty ([],) or it can consist of an element x stuck to the beginning of a list (x : aList.) Do you notice anything weird about this definition of a list? We’re defining a list in terms of its self. It makes complete sense when you think about it. There are only two possible ways a list can exist: either as the empty list, or as a construction of a head and a tail list. The key here is that in a construction of a head and tail, the tail can be any list, either empty, or a construction of its own.\nLet’s try to define a function that maps a list to the count of its elements. Let’s call it count.\n\nThis function should have one parameter: the list of elements we want to count. A list can either be empty, or a construction of at least one element (a head,) and a tail. These are the only two cases we have to deal with since the function only has one parameter, which is a list.\nThe first case is that of an empty list. Easy, an empty list has zero elements:\ncount [] = 0\nThe second case is that of a construction. Hmm… A construction consists of a head (1 element,) and a tail (a list of unknown number of elements.) Ok, so the count of the elements in a construction is 1 + the count of the elements of the tail. Wait… Let’s write that down:\ncount (h : t) = 1 + count(t)\nwhere h is the head, and t is the tail. Together with the empty list case, the whole definition would be:\ncount [] = 0; count (h : t) = 1 + count(t)\nNow that’s just magical. If what’s going on is not obvious, maybe this will help:\nEarlier I explained how functions are equations. We can use this fact to understand how count works by applying it to some list (e.g. oneToFive,) and walking through the evaluation of the result one step at a time.\ncount [1,2,3,4,5] = 1 + count([2,3,4,5])  \ncount [1,2,3,4,5] = 1 + (1 + count([3,4,5]))  \ncount [1,2,3,4,5] = 1 + (1 + (1 + count([4,5])))  \ncount [1,2,3,4,5] = 1 + (1 + (1 + (1 + count([5]))))  \ncount [1,2,3,4,5] = 1 + (1 + (1 + (1 + (1 + count([]))))))  \ncount [1,2,3,4,5] = 1 + (1 + (1 + (1 + (1 + 0)))))  \ncount [1,2,3,4,5] = 1 + 1 + 1 + 1 + 1 + 0  \ncount [1,2,3,4,5] = 5\nAgain, notice how the evaluation would never have ended successfully without a base case (count [].)\nIn general, recursion is defining something in terms of its self. Data structures can be recursive; functions can be recursive; relations in general can be recursive. You’ll find that thinking recursively can make things a lot simpler sometimes.\nYou may have learned about loops in programming (e.g. for and while loops,) and you may be wondering why you would ever use recursion. Well, one reason is that Haskell and some other functional languages don’t have any looping constructs built into them. Another reason is that using recursion lets you use the equational reasoning we applied earlier when evaluating count(oneToFive) in all sorts of situations, saving you some brain power. Most importantly, though, I think recursion makes it easy to describe things clearly and concisely, which results in more understandable code.\nHigher-Order Functions\nYet another scary term.\nLet’s imagine a scenario where you have a list of integers, and you want to increment each of them. You define a function incrementAll that does just that:\nincrementAll [] = []; incrementAll (h : t) = h + 1 : incrementAll t\nEasy. To increment all the elements of an empty list, you do nothing with it. To increment all the elements of a construction, you increment its head, and stick it to all the incremented elements of the tail. Great. You later require a function to decrement all the integers in a list, so you define decrementAll:\ndecrementAll [] = []; decrementAll (h : t) = h -1 : decrementAll t\nLater, you require another function to multiply the integers by two, so you define doubleAll:\ndoubleAll [] = []; doubleAll (h : t) = h * 2 : doubleAll t\n… There’s a problem here. We’re duplicating a lot of code, which quickly becomes boring. If you look at these three functions, you notice they’re essentially the same function, but they map the elements of the list differently. All three walk through the list recursively, and transform its elements using some function (e.g. increment.) How do we solve this problem?\nRemember what I said about functions having types of their own? Functions can be divided into sets. There’s the set of functions from integers to integers (Int -&gt; Int,) the set of functions from lists of integers to integers ([Int] -&gt; Int,) and so on. We can think about a function as we would an integer; we can pass an integer as an argument to a function, right? Why not do the same with functions?\nLet’s define a function transform that walks through a list, and transforms each element using a function f that we pass as an argument:\ntransform f [] = []; transform f (h : t) = f(h) : transform f t\nLooks awfully similar to the previous functions, but with the extra parameter f. What we’re doing here is that we’re leaving the function used to transform the elements up to the user of transform. All we do in transform is traverse a list, and apply a given function f to its elements. Let’s try using it with increment and oneToFive:\ntransform increment oneToFive\nyields [2,3,4,5,6]. Awesome! Note that increment and oneToFive are both arguments of transform. We are not applying increment to oneToFive. Now let’s try transform with decrement:\ndecrement x = x - 1\n \ntransform decrement oneToFive\nyields [0,1,2,3,4]. It works! We can do the same with doouble, but you get the point. transform can be used with any function as long as its type is compatible with the type of the elements of the list.\nA higher-order function is simply a function that takes a function as an argument. So transform is a higher-order function. Many of these functions are available in Haskell by default. For example, filter can be used to filter a list:\nfilter even oneToFive\nThis will filter the even numbers in oneToFive and yield [2,4]. even is a simple function that takes an integer, and yields True if it’s even, and False if it isn’t. The filter function uses the function you pass it to test the elements of the list, and keeps the elements that pass the test.\nAnother popular function is foldl (short for “fold left,”) which “folds” a list as if folding a long piece of cardboard starting from the left and reducing it to a small piece. For example, we can use foldl to find the sum of all the elements of a list of integers:\nfoldl (+) 0 oneToFive\nyields 15. foldl takes three arguments: the function used to calculate the result of every fold, the initial value to start folding with, and the list to fold. This is a bit abstract. If you know about for loops, you can think of folds as loop iterations, where in each iteration, you change the value of some accumulator. In the case of folding a list of integers to find their sum, the accumulator would be the sum, which starts at zero, and then increases with every number you pass in the list to finally be returned as the sum of all the numbers in the list.\nNotice there’s something peculiar about the function we passed to foldl. As the function used to calculate the accumulator in each fold, we used (+). This is possible because operators in Haskell are really just functions of two parameters. To multiply two integers, you pass them as arguments to the (*) function:\n2 * 4\n \n(*) 2 4\nboth yield 8.\nHigher-order functions aren’t limited to lists. They can be used in all sorts of situations (e.g. asynchronous programming,) and they are a good tool for abstraction. We can generalize a formula by making some part of it unknown, and allowing it to be passed as an argument.\n\nWe’ve gone through many concepts so far. We’ve built a pretty good intuition about functions, and we’ve seen how powerful they are. We’ve also discussed types briefly, but now I’d like you to run these lines in the REPL:\nf :: Int -&gt; Int -&gt; Int; f x y = x * x + y  \n:t f  \n:t (f 2)\n \n:t (+)  \n:t (1+)\n \n:t length  \n:t map\nTake a quick look at the result of each of these. :t is a command you can use in the REPL to get the type of an expression. But what’s with these types? There are lots of a’s and fat arrows (=&gt;.) What do they mean? And the question about multiple arrows in the types of functions of multiple parameters remains unanswered! What have we been doing all this time!\nCurrying\nThe result of :t f is Int -&gt; Int -&gt; Int, which is strange. The result of :t (f 2) is Int -&gt; Int, which is even stranger. Doesn’t f take two arguments? Well, that’s the key: f takes two arguments, but (f 2) is another function that takes only one:\nf 2 3\n \n(f 2) 3\nboth yield 7. You can even assign a name to (f 2):\ng = (f 2)\nNow g is a function just like (f 2). It takes one integer argument, and yields an integer, so g 3 will yield 7. Since we’re applying f to only one argument, the value we provide will be used in place of the first parameter (x.) The resulting function takes the one remaining argument (y,) and finally yields an integer.\nCurrying (named after Haskell Curry) is making a function yield another function when it’s not applied to enough arguments. If a curried function takes 5 arguments, applying it to 3 arguments will yield a function that takes 2 arguments. In Haskell, all functions are curried, which allows us to write more concise code. For example, instead of writing a function that sums the elements of a list as:\nsum list = foldl (+) 0 list\nwe can just write:\nsum = foldl (+) 0\nSince foldl takes 3 arguments, and we’re applying it to 2 arguments, it will yield a function that takes one argument (the last parameter, a list,) and finally yields the sum.\nAnother example would be to use curried functions as arguments to higher-order functions. Remember transform? There’s actually a function just like it already defined called map that transforms every element of a list using some function you pass:\nincrement x = x + 1\n \nmap increment oneToFive\nwill yield [2,3,4,5,6]. The addition operator is a function, though, and all functions are curried. So we can apply (+) to one argument, and get a function that takes one argument. So increment can be rewritten as:\nincrement = (+1)\nwhich is cool. But now we don’t really need increment since we have a clear simple way to get a function that increments a number. So a function incrementAll that increments all the elements in a list can be written as:\nincrementAll = map (+1)\nThis works because map has two parameters (a function and a list,) and we’re only applying it to one argument. This is just beautiful.\n\nAlright, we’ve discovered the secret of functions of multiple parameters. Now, let’s answer the question about all the a’s and b’s in types like :t map:\nmap :: (a -&gt; b) -&gt; [a] -&gt; [b]\nParametric Polymorphism\nWhat a tongue twister.\nIn previous sections, we looked at cases where you can generalize a definition by making some part of it unknown. Types are no different; if you take a look at the list type [] you might notice that it’s missing something: the type of elements within it. In order to create a list, you must make it a list of something, so you can think of the list type as a function on the type level that has a type parameter. So to construct a list type that has elements in it, you must pass some type to []. The type [Int] is Int passed to [] as an argument. Because [] itself isn’t a complete type and it has a type parameter, it is called a type constructor.\nBecause a list can come in many forms (e.g. [Int], [Float], and [String],) we say it’s polymorphic. Since the type argument is the part that varies among lists, we say it’s parametrically polymorphic.\nThe same idea applies to functions; in :t map, the type of the list doesn’t matter, because the operations we perform on the list require no knowledge of the type. That is why you see [a] and [b] in :t map. Both a and b can vary between different applications of map, which is why they’re called type variables, and map is called a polymorphic function. For example:\nmap  (+1) oneToFive\nHere, a is Int, and b is Int. So map has the type:\nmap :: (Int -&gt; Int) -&gt; [Int] -&gt; [Int]\nThis also shows that a and b are not necessarily different types. Here’s another example:\nnames = [&quot;John&quot;, &quot;Jane&quot;, &quot;Jackson&quot;]\n \nmap  length names\nHere a is String, and b is Int. So the type of map is:\nmap :: (String -&gt; Int) -&gt; [String] -&gt; [Int]\nThis polymorphism becomes really useful when you want a function or a type to work with multiple types. If you were defining a list data structure, you shouldn’t care about the type it’ll be used with, because you would want it to be usable with any type. It’s also great that the Haskell compiler can figure out types on its own, so you don’t need to think about it too much. Simply define a function, and look at the type that the compiler has given to it, it’s usually the most generic type possible.\nType Classes\nCheck out the type of (+):\n(+) :: Num a =&gt; a -&gt; a -&gt; a\nOk, we understand from a -&gt; a -&gt; a that it’s a function that takes two a’s and yields an a. But what’s with the Num a =&gt; at the beginning?\nImagine you’re defining the (+) function. Would you want it to be usable with any type? I think not. You would want (+) to be usable with types that have properties of numbers (or I hope so, at least.) That’s what the Num a is for. It limits a to types that are number-like. But what is Num?\nIn order for something to be number-like, it has to support some operations like addition (+,) multiplication (*,) and a few others. If a type can be operated on using these functions, we consider it number-like.\nIn Haskell, you can define a type class containing the specification of functions that a type needs to support in order to be considered a member of some class of types.\nThe definition of the Num type class looks like this:\nclass  Num a  where  \n    (+), (-), (*) :: a -&gt; a -&gt; a  \n    negate :: a -&gt; a  \n    abs :: a -&gt; a  \n    signum :: a -&gt; a  \n    fromInteger :: Integer -&gt; a  \n    x - y = x + negate y  \n    negate x = 0 - x\nSome of the functions every Num has to support aren’t concretely defined inside the type class. Functions like (*) and abs are left up to the author of a to define, so only their types are specified. Other functions like (-) have a default concrete definition, given by x — y = x + negate y, so the author of a needs only to define negate for (-) to become available.\nLet’s define our own type class called FillStatus. Any member type of FillStatus must support a function isEmpty that tells us whether the argument is empty or not:\nclass FillStatus a where isEmpty :: a -&gt; Bool\nSo isEmpty takes an a and returns a Bool (True or False.) Now let’s make lists satisfy the requirements of FillStatus by defining an instance of FillStatus for [a]:\ninstance FillStatus [a] where isEmpty [] = True; isEmpty _ = False\nThe definition of isEmpty for lists is pretty straightforward: a list is empty if it’s the empty list ([]) and it’s not otherwise (that’s what the underscore means.) Awesome! Now isEmpty [] yields True, and isEmpty [1,2,3] yields False!\nWe can provide an instance of FillStatus for any type we want (as long as it makes sense.) Type classes are similar to interfaces in other programming languages like Java, but there’s a key difference between the two: you don’t need to specify which type classes a type belongs to while defining it. Int is already defined, and we can still create an instance of FillStatus Int without modifying the definition of Int.\nConclusion\nWe’ve looked at many functional programming concepts in this article. However, this is by no means a comprehensive explanation of FP or Hakell. It is an introduction and a quick overview. After understanding the contents of this article, you should be able to read a lot of Haskell code, and write code in a functional way. I haven’t mentioned things like state, Haskell’s relationship with category theory , or even fundamental things like anonymous functions because this article is long as it is. But I’ll be writing more in the future."},"Blog/Machine-Learning---From-Zero-to-Slightly-Less-Confused":{"title":"Machine Learning - From Zero to Slightly Less Confused","links":[],"tags":[],"content":"When I started my computer science studies three years ago, machine learning seemed like one of those tools that only brilliant scientists and mathematicians could understand (let alone use to solve day-to-day problems). Whenever I heard the words “Machine Learning”, I imagined a high tower with dark clouds above it, and a dragon guarding it. I think the main reason for this irrational fear is that the field is an intersection of so many disciplines that I had no idea about (e.g. statistics, probability, computer science, linear algebra, calculus, and even game theory).\nI know it’s not just me. It’s no wonder people are afraid of Machine Learning, people don’t like Math! Even though understanding some of the very basic Mathematics behind Machine Learning will not only give you a good sense of how it works, but it’ll get you far as a Machine Learning practitioner. And who knows, maybe you’ll grow to like the Math, like me.\nIn this article, I’ll attempt to give you a better understanding of what Machine Learning really is, and hopefully get rid of any fear of the subject you’ve been building up. Getting started solving real world problems using Machine Learning can be much easier than many are led to believe.\nMachine Learning (ML) is the science of making machines perform specific tasks, without explicitly writing the algorithm for performing the tasks. Another definition would be making the machine learn how to perform some task from experience, taking into account some performance measure (how well it performs the task).\nLet’s consider these two popular problems:\n\n\nGiven some features of a breast tumor (i.e. its area and smoothness), predict whether the tumor is malignant or benign.\n\n\nGiven the monthly income of a house in California, predict the house’s price.\n\n\nProblem 1: Tumor Classification\nLet’s see. We are using two variable features of a tumor to determine whether it is malignant or benign, how can we go about solving this problem?\nWell, we can try to come up with some logic to decide the class of the tumor. Maybe something like:\ndef tumor_class(tumor):\n      area = tumor[0]\n      smoothness = tumor[1]\n      if area &lt; 110 and smoothness &lt; 0.07: \n        return &#039;Malignant&#039;\n      elif area &gt; 110 and smoothness &lt; 0.07:\n        return &#039;Benign&#039;\n      elif area &lt; 110 and smoothness &gt; 0.07:\n        return &#039;Malignant&#039;\n      else:\n        return &#039;Benign&#039;\n\nYou can find and experiment with all the code on Google Colab.\n\nBut how can we know these threshold values (110 and 0.07)? How accurate is this algorithm? What if we had to use more than two features to predict the tumor’s class? What if a tumor could belong to one of three or four classes? The program would become way too difficult for a human to write or read.\nLet’s say we have a table of 569 breast tumors that has three columns: the area, the smoothness, and the class (type) of tumors. Each row of the table is an example of an observed tumor. The table looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAreaSmoothnessClass594.20.124801.01007.00.100100.0611.20.084581.0………\n\nA row of the table can be called an example, an instance, or a tuple. A column of the table can be called a feature.\nIn ML, the feature we want to predict is often called the target, or label.\n\nNever mind the measurement of the area and smoothness, but pay attention to the Class column. Class 1 represents “Malignant”, and class 0 represents “Benign”.\nAlright, now that we have some data, we can plot it and see if that’ll help us:\n\nThe X axis represents the area of the tumor, while the Y axis represents its smoothness. Each data point (tumor) is colored orange if it’s malignant, or green if it’s benign.\nNotice how the two classes are roughly separated. Maybe we can draw a line that (roughly) separates the two classes (any tumor under the line is malignant, and any above the line is benign):\n\nBut what about the tumors that are misclassified? There are green points under the line, and orange points above it. If drawing a straight line is all we’ll do, then we need to modify the line’s equation in order to minimize the error.\nAny straight line has the form: y = ax + b. Which means we can keep modifying a and b until the number of misclassified tumors is at its minimum. This is called the training process. We are using our data (experience) to learn the task of predicting tumor classes, with regard to how often we misclassify tumors.\n\na and b are called weights. a and x can be vectors depending on the number of features we’re using to predict y. In our case, the line’s equation can be written as y = a[1]*x[1] + a[21]*x[2] + b, where a[1] is the weight of the first feature (x[1], the area), and a[2] is the weight of the second feature (x[2], the smoothness).\n\nThe goal of the training process is to learn a function of the training features that predicts the target. Concretely, the function learned from training on our tumor data is a function that takes two arguments (area and smoothness), and returns the class of the tumor (0 or 1). This function is called the model.\nOnce the model is trained, we can start making predictions on new (previously unseen) breast tumors.\nThis entire process can be done in 13 lines of simple Python code:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\n \ncancer_data = load_breast_cancer()\n \n# Despite its name, LogisticRegresssion is actually a classification model\nclassifier = LogisticRegression(solver=&#039;lbfgs&#039;, max_iter=5000)\nclassifier.fit(cancer_data.data[:,[3, 4]], cancer_data.target)\n \ndef tumor_type(tumors):\n  y = classifier.predict(tumors)\n  print([&#039;Malignant&#039; if y == 1 else &#039;Benign&#039; for y in y])\n  \ntumor_type([\n    [50, 0.06],\n    [1500, 0.1], # Prints out: \n    [200, 0.04]  # [&#039;Malignant&#039;, &#039;Benign&#039;, &#039;Malignant&#039;]\n])\n\nThis example uses Scikit-learn, a very popular Python ML library. But you’re not limited to Scikit-learn or Python. You can do ML in any language you like. R and MatLab are pretty popular choices.\n\n\nIn ML, the problems where your goal is to predict a discrete label (e.g. spam/not spam, male/female, or malignant/benign) are called classification problems. Our tumor classification problem is more specifically a binary classification problem (the output is one of only two classes).\nSince we used a line to separate the two classes and predict the class of any new tumor, our model is called a linear model.\nNow let’s look at a regression problem.\nProblem 2: Predicting House Prices\nSuppose that you have a dataset that contains 17,000 records of houses in California. And given the median monthly income of of a city block, you are tasked with predicting the median house value in that block.\nLet’s start by plotting the data that we have:\n\nThe X axis represents the median block income in thousands, and the Y axis represents the median house price of the block (in U.S Dollars).\nNotice how we can roughly represent the relation between the income and price as a straight line:\n\nWhat we can do now is modify our line’s equation to get the most accurate result possible.\nAgain, we can do all of this with a few lines of Python code:\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n \nhouse_data = pd.read_csv(&#039;sample_data/california_housing_train.csv&#039;)\nhouse_target = house_data[&#039;median_house_value&#039;]\nhouse_data = house_data[&#039;median_income&#039;].to_numpy().reshape(-1, 1)\n \nregressor = LinearRegression().fit(house_data , house_target )\n \ndef house_price(incomes):\n  print(regressor.predict([[i] for i in incomes]).tolist())\n \nhouse_price([2, 7, 8]) \n# Prints out: [127385.28173581685, 338641.43861720286, 380892.66999348]\nNow you might be saying “A straight line doesn’t fit this data!”, and I would agree with you. There are many things we can do to improve the performance of this model, like getting rid of some of the outliers in the data:\n\nWhich affects the training process. We could look for a feature that better relates to the price, or use multiple features of the houses to get a multidimensional line. We can also scale down the data to speed up the training process. We can even use a different kind of model.\nMany steps can be takes before even starting to train a model that will immensely improve its performance (i.e. feature engineering and preprocessing). One might even decide that they don’t have the right data for their purposes, so they start collecting it.\nThis problem is an example of a regression problem, which is a problem where the result of the prediction is a value belonging to a continuous range of values (e.g. price in Dollars, age in years, or distance in meters).\n\nThe two problems we looked at are examples of supervised ML problems. Which are essentially the problems where the data used is labeled, meaning the target feature’s values are known in the training data (e.g. our tumor data was labeled malignant/benign, and our house data was labeled with the price). What would we do if our data isn’t labeled?\nI hope you’re starting to see the big picture. ML is wide and deep, and it can get very difficult. But the basics are just that: basics.\nIf I’ve managed to spark your interest in the subject, then I’d like to point you to a few places where you can learn much more:\n\nStanford’s Machine Learning course on Coursera, which is also available on YouTube\nKhan Academy for all the basic Math\nGoogle’s Machine Learning Crash Course\nO’Reily: Data Science from Scratch\nO’Reily: Introduction to Machine Learning with Python\nGoogle Colaboratory: a fully hosted Jupyter environment (you don’t need to install or set up anything, just do it all here)\n\nI found these resources very helpful. Pick and choose whichever feels comfortable for you.\nI hope you found this article helpful, and I would love to read your opinions in the comments."},"Blog/Parsing-The-World-with-Rust-and-POM":{"title":"Parsing The World with Rust and POM","links":[],"tags":[],"content":"As programmers, we spend a lot of time dealing with strings of text. Very often, we receive text as input from users, or we read text files and try to understand their content.\nIn many cases, we use regular expressions to see if the text matches a certain pattern, or to extract some information from the string. For example, if you receive a username as form input, and you want to make sure that it doesn’t contain any spaces, you can use a regex like &quot;^\\S+$&quot; to match a string with one or more non-white space characters (^ denotes the beginning of the string, \\S denotes a non-white space character, + denotes repetition for one or more times, and $ denotes the end of the string). You might even write a function like:\nfn has_whitespaces(s: String) -&gt; bool {  \n s.contains(‘ ‘)  \n}\n\nCan you spot the bug in the code?\n\nBut what if you’re trying to parse the contents of more complex strings. Say, a JSON string, a CSV (Coma-Separated Values) file, or even a program? If you don’t use existing library functions, you’re going to have a hard time using regular expressions and string methods.\nIn this article, we’ll be exploring the POM library, which offers a really cool interface for intuitively defining and combining parsers. Using this library, you can conveniently define the entire grammar of a formal language (perhaps your own new programming language?)\nPOM is an implementation of a PEG (Parsing Expression Grammar,) which is a definition of a formal language in terms of symbols, string expressions, and rules.\nFor example, if we were to define a rule for parsing a variable definition such as: let x = 1;, we would say that let is a sequence of symbols, =, ;, and spaces are symbols, and x and 1 are string expressions. A variable definition consists of the let sequence, followed by a space, followed by a valid identifier (x, for example,) followed by zero or more spaces, followed by the = symbol, followed by zero or more spaces, followed by a valid expression (1 in this example,) followed by zero or more spaces and the ; symbol. A bit verbose, isn’t it? Such are the definitions of formal languages.\nThankfully, POM provides a very declarative way of defining these rules. It allows us to define different rules for parsing each small piece of text, and then combine them using arithmetic and logical operators to form more complex rules. These operators are called parser combinators.\nPOM defines a type called: Parser, which is used to encode parsing rules. A parser can be constructed using many of the predefined parsers that the library provides, such as the sym and seq functions. The above example of a variable definition can be expressed in POM as:\nuse pom::char_class::;\nuse pom::parser::;\nfn variable_def&lt;&#039;a&gt;() -&gt; Parser&lt;&#039;a, u8, (String, u32)&gt; {  \n  let valid_id = (is_a(alpha) + is_a(alphanum).repeat(0..))  \n    .map(|(first, rest)| format!(&quot;{}{}&quot;, first as char, String::from_utf8(rest).unwrap()));  \n  let valid_expr = one_of(b&quot;0123456789&quot;)  \n    .repeat(1..10)  \n    .convert(String::from_utf8)  \n    .convert(|s| s.parse::&lt;u32&gt;());  \n  seq(b&quot;let&quot;) * sym(b&#039; &#039;) * valid_id - sym(b&#039; &#039;) - sym(b&#039;=&#039;)   \n    - sym(b&#039; &#039;).repeat(0..) + valid_expr - sym(b&#039;;&#039;)  \n}\nLet’s break down the code:\n1 —  We import all the contents of pom::char_class, which exports \npredicates such as alpha, and alphanum. We use these predicates to \ntest the type of input characters.\npom::parser exports seq, and sym among other functions that return Parsers.\nuse pom::char_class::*;  \nuse pom::parser::*;\n2 —  The variable_def function takes no arguments, and return a Parser \nwith the lifetime &#039;a that accepts u8s (character bytes) as input, and \noutputs a tuple of (String, u32). The tuple encodes the variable’s \nidentifier, and its value (we limit valid values to 9-digit positive \nintegers for simplicity’s sake.)\nfn variable_def&lt;’a&gt;() -&gt; Parser&lt;’a, u8, (String, u32)&gt;\n3 —  A valid variable identifier consists of at least one alphabetic character, followed by zero or more alphanumeric characters. We use the is_a parser to test that the first character of the variable id is_a(alpha), and that the rest of the id is_a(alphanum).\nNotice how the + operator is used to combine the two parts of the variable identifier. It returns a tuple containing the results of both operands, which we then destructure in the map method call. map is used to transform the result of a parser, and return a new parser. In this case: (is_a(alpha) + is_a(alphanum).repeat(0..)) returns a Parser&lt;’a, u8, (u8, Vec&lt;u8&gt;)&gt; (the first character and the rest of the characters,) which we then transform into a Parser&lt;&#039;a, u8, String&gt; (the whole identifier) by concatenating the first character and the rest of the characters using format!.\nlet valid_id = (is_a(alpha) + is_a(alphanum).repeat(0..))  \n    .map(|(first, rest)| format!(&quot;{}{}&quot;, first as char,                  String::from_utf8(rest).unwrap()));\n4 —  A valid expression is defined as 1 to 9 numbers converted to a String which is then parsed as a u32. convert is used here instead of map because String::from_utf8 returns a Result, so the parser wouldn’t match the expression if the result of String::from_utf8 is Err.\nlet valid_expr = one_of(b&quot;0123456789&quot;)  \n    .repeat(1..10)  \n    .convert(String::from_utf8)  \n    .convert(|s| s.parse::&lt;u32&gt;());\n5 —  The returned parser is a combination of the symbols mentioned above mixed with some spaces, and a valid identifier and expression. The * operator combines two parsers and returns the result of the right-hand parser. So Parser&lt;&#039;a, u8, T&gt; * Parser&lt;&#039;a, u8, U&gt; = Parser&lt;&#039;a, u8, U&gt;. We also see the - operator in use, which combines two parsers and returns a parser with the value of the left-hand parser. These operators follow the same precedence rules as normal arithmetic operators in Rust.\nseq(b&quot;let&quot;) * sym(b&#039; &#039;) * valid_id - sym(b&#039; &#039;) - sym(b&#039;=&#039;)   \n    - sym(b&#039; &#039;).repeat(0..) + valid_expr - sym(b&#039;;&#039;)\n\nTake a look at this handy table of parser combinators.\n\nNow to test our parser:\n#[test]  \nfn test_variable_parser() {  \n  let valid_variable_byte_string = b&quot;let v1 = 42;&quot;;  \n  let parsed_variable =   variable_def().parse(valid_variable_byte_string);  \n  assert_eq!(parsed_variable, Ok((&quot;v1&quot;.to_string(), 42)));\n \n  let invalid_variable_byte_string = b&quot;let nosemicolon = 42&quot;;  \n  let parsed_variable =   variable_def().parse(invalid_variable_byte_string);  \n  assert_eq!(parsed_variable, Err(pom::Error::Incomplete));\n \n  let invalid_variable_byte_string = b&quot;let morethan9digits =  1234567890;&quot;;  \n   let parsed_variable =  variable_def().parse(invalid_variable_byte_string);  \n  assert_eq!(  \n    parsed_variable,  \n    Err(pom::Error::Mismatch {  \n      message: &quot;expect: 59, found: 48&quot;.to_string(),  \n      position: 31  \n    })  \n  );  \n}\nUsing POM to parse a variable definition of a positive integer value might seem overkill, but try to imagine using it to parse an entire programming language.\nI hope you found this article helpful. Now enjoy parsing every string in your way.\nSpecial thanks to Junfeng Liu and all contributors for creating an amazing library. You deserve a cookie 🍪"},"Blog/RxJS-From-Scratch":{"title":"RxJS From Scratch","links":[],"tags":[],"content":"\nWhen looking at RxJS for the first time, some of the concepts might seam confusing, and strange, that’s because the library borrows ideas from two fundamentally different programming paradigms, object-oriented, and functional programming. In this post, I assume you know a little bit of both. A good understanding of higher order functions, and class-oriented programming can help you tremendously here.\nRxJS is a JavaScript library for reactive programming, which is a programming paradigm concerned with dealing with change (reacting to it), RxJS enables this by implementing the Observer pattern, where observers can subscribe to an observable to be notified whenever the observable emits a new value.\nThe Observer Pattern\nLet’s consider YouTube as an example implementation of this pattern, a viewer can subscribe to a channel in order to receive notifications whenever that channel releases a new video, in other words, the viewer observes that channel, and watches for new videos. In this case, the viewer is an observer, the YouTube channel is an observable, and the observable emits new videos to it’s subscribers.\nOne very important thing to notice is that the subscriber doesn’t know when the YouTube channel will release a new video, so it wouldn’t be a good idea for them to sit for hours in front of their screen waiting for a new release, instead, they can do other things, and when a new video comes out, they can watch it (react to it). A simplification of the above example might look like this in RxJS:\nFirst, we create the channel object, which is of type Observable. The channel has 3 videos to release, and it sends them to each subscriber through the next method.\nWe then create a viewer object, which implements a next method, which handles any incoming videos. Finally, we subscribe the viewer to the channel using the subscribe method of the observable.\nOperators\nAn operator is a function that can be used to operate on values emitted by an observable. For example, an operator can be used to filter the values emitted by the observable, or transform the values into something else. Consider this simple and rather trivial example:\n\nRxJS comes with a healthy amount of readily available operators, here we import two of which, filter, and map.\nThe Rx.of function is a function that creates an observable that emits all the provided values in sequence, which means that the numbers observable will emit the values 1 through 6.\n\nNote: Observables van be created in many ways using functions such as from and fromEvent, which you can read about here.\n\nisEven is a simple function that tells us whether a number is even or not..\nevenNumbers is an observable that emits every even number emitted by numbers. The filtering of numbers is done by the filter operator, which we imported from rxjs/operators. The filter operator is very similar to the Array.filter method in that it takes a predicate as an argument, where the predicate is a function that returns either true or false (.e.g isEven). The predicate is applied to every value emitted by the source observable (.i.e numbers), and only the values that match the predicate are re-emitted.\nobviouslyCorrectStatements is an observable that emits a console-friendly representation of evenNumbers, where it transforms every even number into a string stating that the number is even. This is achieved by the map operator, which is very similar to Array.map, in that it takes a function that accepts value, and returns a different representation of that value (it maps every value to a new value). The passed function is then applied to every value emitted by the source observable (.i.e. evenNumbers), and every transformed value is emitted.\n\nNotice how we pass the operators to the pipe method, which is essentially a way to chain operators to achieve the desired result. For example, this code is roughly equivalent to the code above:\nThe only difference being that we skipped creating the evenNumbers observable, and went straight to having an observable of trivial statements about even numbers.\nThe pipe method takes a list of operators, where every emitted value is passed to the first operator in the list, and the result of that operator is passed to the next operator, and so on.\nA useful way of visualizing a pipe is as a literal pipe that data goes through:\n\nThere are many operators that can be immensely helpful, such as flatMap, and reduce, but we can’t cover them all in one post, so I recommend reading about them early on to save a huge amount of time. You can find example uses of each operator here.\nNote: The only requirement a function needs to fulfill is to return an observable, which means that you can create your own operators.\nConclusion\nRxJS is a great library that combines object-oriented and functional programming to strike a reactive balance between the two, and it does it elegantly. I think it’s a very useful tool for any JavaScript developer to master.\nI hope you found this post useful. Have fun doing reactive programming!"},"Blog/The-Scala-Collections-Library":{"title":"The Scala Collections Library","links":[],"tags":[],"content":"\nOne of my favorite features of Scala is the amazingly rich standard collections library. Normally, languages try not to make any assumptions about your preferences of the algorithms used to implement certain operations on data structures, so they don’t provide implementations for these operations at all. Consequently, you either implement your own structures/operations and endure the debt of maintaining them, or you use an external library that implements its own data structures. But what’s the point of having a standard library of collections if the mismatch between everyone’s most basic data representations is inevitable?\nLuckily, the Scala collections library contains a plethora of data structures and generic operations that are hard to find in the standard libraries of other languages. Combined with the expressive nature and type safety of Scala, manipulating complex data is a blast.\n\nThis article assumes the reader is using Scala 2.13.0\n\nOverview of scala.collection\nThe scala.collection package contains definitions of many abstract structures, such as Seq (sequence) and Map , and Iterable, among some generic operations. It also contains mutable and immutable sub-packages which contain concrete implementations of these abstract data structures. For example, immutable.List is an extension of Seq, and immutable.HashMap is an extension of Map. We’ll discuss these types of collections later on.\nIterable\nIterable is the base of all collections, which has one abstract method: iterator, and one type argument: A , the type of elements in the collection. It defines many useful operations such as map, filter, and reduce, so you can expect these operations to be present for all collections. This makes it really easy to define functions of Iterables which can operate on many types of collections. For example:\nscala&gt; // Takes an iterable of Strings\n \nscala&gt; def longerThan5(it: Iterable[String]) = it.filter(_.length &gt; 5)  \nlongerThan5: (it: Iterable[String])Iterable[String]\n \nscala&gt; // Can be applied to a List[String]\n \nscala&gt; longerThan5(List(&quot;John&quot;,&quot;Alice&quot;,&quot;Edward&quot;,&quot;Victoria&quot;))  \nres0: Iterable[String] = List(Edward, Victoria)\n \nscala&gt; // Can be applied to a Set[String]\n \nscala&gt; longerThan5(Set(&quot;John&quot;,&quot;Alice&quot;,&quot;Edward&quot;,&quot;Victoria&quot;))  \nres1: Iterable[String] = Set(Edward, Victoria)\n\nThe type argument A of Iterable is covariant, meaning that Iterable[Int] is a subtype of Iterable[Number] for example.\n\nValues of type Iterable can be created using Iterable.apply:\nscala&gt; val iterable = Iterable(1,2,3)  \niterable: Iterable[Int] = List(1, 2, 3)\nNotice how the actual value of iterable has the type List[Int]. This is the default type that is given to values constructed using Iterable.apply (since Iterable is abstract, and List is a concrete implementation of it.) We’ll take a look at Lists shortly.\nWe can iterate over any Iterables using for:\nscala&gt; for(n &lt;- Iterable(1,2,3)) print(s&quot;$n &quot;)  \n1 2 3\nYou can also use [for](docs.scala-lang.org/tour/for-comprehensions.html) comprehensions to transform and filter the values in the collection and return the resulting collection:\nscala&gt; for(n &lt;- Iterable(1,2,3); if n % 2 == 0) yield s&quot;$n is even!&quot;  \nres5: Iterable[String] = List(2 is even!)\nfor comprehensions are actually syntactic sugar for chains of map, filter, and foreach calls. So the code above is equivalent to:\nscala&gt; Iterable(1,2,3).filter(_ % 2 == 0).map(n =&gt; s&quot;$n is even!&quot;)  \nres6: Iterable[String] = List(2 is even!)\n\nCheck out function shorthand syntax if _ % 2 == 0 looks alien to you.\n\nAny type that implements foreach can be iterated over using for. This gives you the ability to iterate over and transform your own types using very clean syntax. You can also enable the use of yield and if guards if you implement map and filter methods for the type.\nYou can concatenate Iterables using the ++ operator:\nscala&gt; Iterable(1,2,3) ++ Iterable(4,5,6)  \nres0: Iterable[Int] = List(1, 2, 3, 4, 5, 6)\nIterables also have head, tail and isEmpty methods that make recursion pretty convenient:\nscala&gt; val ns = Iterable(1,2,3)  \nns: Iterable[Int] = List(1, 2, 3)\n \nscala&gt; ns.head  \nres0: Int = 1\n \nscala&gt; ns.tail  \nres1: Iterable[Int] = List(2, 3)\n \nscala&gt; def sum(ns: Iterable[Int]): Int =   \n     |  if(ns.isEmpty) 0 else ns.head + sum(ns.tail)  \nsum: (ns: Iterable[Int])Int\n\nIterables actually have a sum method that sums any collection of numbers. How convenient is that?\n\nWhen using head and tail (and some other methods,) you should keep in mind that the collection might be empty. Calling head or tail on an empty collection results in a NoSuchElementException or an UnsupportedOperationException respectively. A type-safe way to get around this problem is to use headOption and match its result to know whether to call tail or not. Calling tail on a collection with one element will result with an empty collection:\nscala&gt; def sum(ns: Iterable[Int]): Int =  \n     |  ns.headOption match {  \n     |   case None =&gt; 0  \n     |   case Some(n) =&gt; n + sum(ns.tail)  \n     |  }  \nsum: (ns: Iterable[Int])Int\n\nThe [_Option_](www.scala-lang.org/api/2.13.0/scala/Option.html) type encodes a value that might not exist, such as the head of an empty collection in this example.\n\nThis pattern of fooOption appears in many places in the collections API. For example, the reduce method assumes the first element of the collection to be the initial value for the accumulator, which is returned at the end:\nscala&gt; Iterable(1,2,3).reduce((a, b) =&gt; a + b)  \nres0: Int = 6\nThe definition of reduce is similar to:\ndef reduce[A](it: Iterable[A], op: (A, A) =&gt; A): A =\n\tif(it.size == 1) it.head   \n\telse reduce(Iterable(op(it.head,it.tail.head))++it.tail.tail,op)\nWhich doesn’t take into account that the collection can be empty. The reduceOption method returns None if the collection is empty. The fold method is similar to reduce, but it takes the initial value of the accumulator as an argument:\nscala&gt; Iterable(1,2,3).fold(1)((a, b) =&gt; a * b)  \nres0: Int = 6\nIterables also support drop, take, dropWhile, and takeWhile:\nscala&gt; val ns = Iterable(1,5,10,12,20,50)  \nns: Iterable[Int] = List(1, 5, 10, 12, 20, 50)\n \nscala&gt; ns.take(2)  \nres0: Iterable[Int] = List(1, 5)\n \nscala&gt; ns.drop(2)  \nres1: Iterable[Int] = List(10, 12, 20, 50)\n \nscala&gt; ns.takeWhile(_ &lt; 20)  \nres2: Iterable[Int] = List(1, 5, 10, 12)\n \nscala&gt; ns.dropWhile(_ &lt; 20)  \nres3: Iterable[Int] = List(20, 50)\nIterables have many convenience methods, such as groupBy, which groups elements of the collection based on some attribute (user age in this example):\nscala&gt; case class User(name: String, age: Int)  \ndefined class User\n \nscala&gt; val users = Iterable(  \n     |  User(&quot;John&quot;, 21),  \n     |  User(&quot;Jane&quot;, 30),  \n     |  User(&quot;Alice&quot;, 21))  \nusers: Iterable[User] = List(User(John,21), User(Jane,30), User(Alice,21))\n \nscala&gt; users.groupBy(_.age)  \nres1: scala.collection.immutable.Map[Int,Iterable[User]] = HashMap(\n\t21 -&gt; List(User(John,21), User(Alice,21)),   \n    30 -&gt; List(User(Jane,30)))\nWe can’t cover all the methods of Iterable (or any subtype of it) in this article, but I encourage you to use the API documentation for reference.\nSeq\nThe Seq type represents an order-preserving sequence of elements. Any subtype of Seq has an apply method that takes an integer index (starting at zero) and returns the element at that location:\nscala&gt; val s = Seq(1, 2, 3, 1, 2, 3)  \ns: Seq[Int] = List(1, 2, 3, 1, 2, 3)\n \nscala&gt; s(2)  \nres0: Int = 3\n \nscala&gt; // But be careful not to access elements out of bounds\n \nscala&gt; s(100)  \njava.lang.IndexOutOfBoundsException: 100  \n  at scala.collection.LinearSeqOps.apply(LinearSeq.scala:118)  \n  at scala.collection.LinearSeqOps.apply$(LinearSeq.scala:115)  \n  at scala.collection.immutable.List.apply(List.scala:82)  \n  ... 36 elided\n \nscala&gt; // Use lift to be safe\n \nscala&gt; s.lift(100)  \nres2: Option[Int] = None\nAll Iterables have a size method, which returns the number of elements in the Iterable. Seqs have another method: length, which does the same thing. Seqs define the size method as an alias to the length method, so they’re exactly the same thing:\nscala&gt; val s = Seq(1, 2, 3)  \ns: Seq[Int] = List(1, 2, 3)\n \nscala&gt; s.length  \nres0: Int = 3\n \nscala&gt; s.size  \nres1: Int = 3\nYou can update the element at a particular index:\nscala&gt; Seq(&#039;a&#039;,&#039;b&#039;,&#039;c&#039;).updated(1,&#039;z&#039;)  \nres2: Seq[Char] = List(a, z, c)\nSeqs can be sorted if there’s an implementation of [Ordering](scala-lang.org/api/2.13.0/scala/math/Ordering.html) for its elements in scope:\nscala&gt; Seq(10,2,4,8).sorted  \nres0: Seq[Int] = List(2, 4, 8, 10)\n \nscala&gt; Seq(&quot;bananas&quot;,&quot;apples&quot;,&quot;oranges&quot;).sorted  \nres1: Seq[String] = List(apples, bananas, oranges)\nYou can append and prepend elements to a Seq:\nscala&gt; val seq = Seq(1,2,3)  \nseq: Seq[Int] = List(1, 2, 3)\n \nscala&gt; 0 +: seq  \nres0: Seq[Int] = List(0, 1, 2, 3)\n \nscala&gt; seq :+ 4  \nres1: Seq[Int] = List(1, 2, 3, 4)\n \nscala&gt; seq ++ Seq(4,5,6)  \nres2: Seq[Int] = List(1, 2, 3, 4, 5, 6)\n \nscala&gt; Seq(-2,-1,0) ++: seq  \nres3: Seq[Int] = List(-2, -1, 0, 1, 2, 3)\nThe performance of these operations depends on the concrete implementation of Seq (the actual value.)Lists are implemented as linked lists in Scala, so they support constant-time prepending of single elements. On the other hand, they take O(n1) time to prepend a list of n1 elements to another list. Accessing elements by index in a List of n elements also takes O(n) time. It’s useful to know this to be able to determine the best data structure to use.\nVectors are Seqs that support constant-time random access/updates of elements, so they’re a good choice if you do a lot more than just iterating over the sequence and prepending elements to the collection.\nList\nLists are implemented as cons lists, which can be defined as:\nsealed trait ConsList[+A]\n\ncase class Cons[A](head: A, tail: ConsList[A]) extends ConsList[A]\n\ncase object Empty extends ConsList[Nothing]\n\nAnd values of ConsList can be constructed like so:\nscala&gt; Cons(1, Cons(2, Cons(3, Empty)))  \nres3: Cons[Int] = Cons(1,Cons(2,Cons(3,Empty)))\nBut what’s cool is that Scala support syntax that allows a value constructor like Cons to be a symbol like ::, so Lists can be constructed in this fashion:\nscala&gt; 1 :: 2 :: 3 :: Nil  \nres4: List[Int] = List(1, 2, 3)\nWhere Nil represents the empty list. This is cool because it allows us to match patterns with lists, for example:\nscala&gt; def sum(xs: List[Int]): Int = xs match {  \n     |   case Nil =&gt; 0  \n     |   case head :: tail =&gt; head + sum(tail)  \n     | }  \nsum: (xs: List[Int])Int\nOf course, you can always just use List.apply to construct and match values:\nscala&gt; List(1,2,3)  \nres5: List[Int] = List(1, 2, 3)\nLazyList\nA LazyList is a List that lazily evaluates its elements. Which means that elements of a LazyList would never be evaluated unless they are used:\nscala&gt; LazyList(1,2,3)  \nres0: scala.collection.immutable.LazyList[Int] =   \nLazyList(&lt;not computed&gt;)\n \nscala&gt; LazyList(1+2&lt;3,false,Seq.range(1,100000).length&gt;300)  \nres1: scala.collection.immutable.LazyList[Boolean] =   \nLazyList(&lt;not computed&gt;)\n \nscala&gt; // But will be evaluated when needed\n \nscala&gt; LazyList(1,2,3).sum  \nres3: Int = 6\nSet\nA Set is a collection of unique elements that can be constructed using Set.apply:\nscala&gt; Set(1,2,3,1,2,3)  \nres0: scala.collection.immutable.Set[Int] = Set(1, 2, 3)\nSets have an apply method that takes an element and returns whether the element is a member of the set or not:\nscala&gt; val ns = Set(10,20,30)  \nns: scala.collection.immutable.Set[Int] = Set(10, 20, 30)\n \nscala&gt; ns(20)  \nres1: Boolean = true\n \nscala&gt; ns(1)  \nres2: Boolean = false\nPerforming any of the Iterable operations will preserve the uniqueness of the elements in the set. Additionally, Sets support + and — methods that perform element insertion and extraction respectively:\nscala&gt; Set(1,2,3) + 4  \nres0: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)\n \nscala&gt; Set(1,2,3) - 3  \nres2: scala.collection.immutable.Set[Int] = Set(1, 2)\nThey also support union and intersect methods:\nscala&gt; Set(1,2).union(Set(2,3))  \nres4: scala.collection.immutable.Set[Int] = Set(1, 2, 3)\n \nscala&gt; Set(1,2).intersect(Set(2,3))  \nres5: scala.collection.immutable.Set[Int] = Set(2)\nSets don not preserve the order in which elements were inserted. But you can use ListSet if order matters to you.\n\nSee also: HashSet and TreeSet.\n\nMap\nA Map is a set of key-value pairs that can be created using Map.apply:\nscala&gt; Map(5 -&gt; &quot;Hello&quot;, 11 -&gt; &quot;Collections&quot;)  \nres0: scala.collection.immutable.Map[Int,String] =   \nMap(5 -&gt; Hello, 11 -&gt; Collections)\n\n-&gt; is actually just a method that returns a tuple of the two operands, so &#039;a&#039; -&gt; 1 returns (&#039;a&#039;, 1).\n\nYou can use the apply method to get the value of a key:\nscala&gt; val lengths = Map(5 -&gt; &quot;Hello&quot;, 11 -&gt; &quot;Collections&quot;)  \nlengths: scala.collection.immutable.Map[Int,String] = Map(5 -&gt; Hello, 11 -&gt; Collections)\n \nscala&gt; lengths(5)  \nres2: String = Hello\n \nscala&gt; lengths(100)  \njava.util.NoSuchElementException: key not found: 100  \n  at scala.collection.immutable.Map$Map2.apply(Map.scala:274)  \n  ... 36 elided\nSimilarly to Seqs, you can use lift to avoid NoSuchElementExceptions:\nscala&gt; lengths.lift(11)  \nres6: Option[String] = Some(Collections)\n \nscala&gt; lengths.lift(99999)  \nres7: Option[String] = None\nMaps also support + and — operators:\nscala&gt; lengths + (4 -&gt; &quot;Maps&quot;)  \nres9: scala.collection.immutable.Map[Int,String] =   \nMap(5 -&gt; Hello, 11 -&gt; Collections, 4 -&gt; Maps)\n \nscala&gt; lengths - 5  \nres11: scala.collection.immutable.Map[Int,String] =   \nMap(11 -&gt; Collections)\nAdding a new key-value pair will overwrite any existing pairs with the same key:\nscala&gt; lengths + (5 -&gt; &quot;Scala&quot;)  \nres13: scala.collection.immutable.Map[Int,String] =   \nMap(5 -&gt; Scala, 11 -&gt; Collections)\nYou can iterate over the key-value pairs of a map using for(or any method on Iterable):\nscala&gt; for((k,v) &lt;- lengths) println(s&quot;$k\\t-&gt;\\t$v&quot;)  \n5       -&gt;      Hello  \n11      -&gt;      Collections\n\nSee also: ListMap, HashMap, TreeMap.\n\n\nSo far, we’ve looked at List, LazyList, Map and Set, which are all immutable collections. The other side of the collections library: scala.collection.mutable, contains data structures similar to these(plus some new ones, like Stack,) but ones you can mutate in place (without creating new collections.) We also haven’t looked at Queues, which are also in collection.immutable. It would be difficult to cover all of this in one article (or one book,) but again, I urge you to refer to the API documentation for a complete view of the collections library.\nI hope you found this introduction to be helpful. My aim was to help anyone new to Scala get started using some of the most useful functionality of the collections library (and also for me to gush over some of its really cool features.)"},"index":{"title":"Hi 👋","links":[],"tags":[],"content":"Co-founder at Open.cx (YC W24).\n\nRead Blog\n\n\nFollow on X\n\n\nConnect on LinkedIn\n\nWhat I Do\nmindmap\n  root((Experience))\n    ((Backend Dev))\n      ((Scala))\n        (Typelevel)\n        (Tapir)\n        (Caliban)\n\n      ((Kotlin))\n        (Spring Boot)\n        (Ktor)\n        (Jooq)\n\n      ((Elixir))\n        (Ecto)\n        (Absinthe)\n        (Phoenix)\n\n      ((JS/TS))\n        (Express)\n        (NestJS)\n        (tRPC)\n\n    ((Frontend Dev))\n      (React)\n      (Next.js)\n      (Shadcn)\n      (RxJS)\n      (Tailwind)\n      (TyprScript)\n      (ReScript)\n\n    ((DevOps))\n      ((Kubernetes))\n      (GitHub Actions)\n\n      ((Observability))\n        (OpenTelemetry)\n        (Grafana Stack)\n\n      ((Google Cloud))\n        (Cloud Run)\n        (Compute Engine)\n        (Cloud SQL)\n        (Memorystore)\n        (Pub/Sub)\n        (Cloud Storage)\n        (Cloud Batch)\n        (Dataform)\n\n    ((Databases))\n      (PostgreSQL)\n      (MySQL)\n      (Redis)\n      (Elasticsearch)\n      (BigQuery)\n"}}